\documentclass[english, LaM, oneside]{sapthesis}
%Master's (laurea specialistica) thesis: LaM 
%PhD's thesis: PhD 
%\usepackage[italian]{babel} %use this package for a thesis written in Italian
\usepackage[utf8]{inputenx}
\usepackage{indentfirst}
\usepackage{microtype}
\usepackage{nomencl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{bbold}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage[section]{placeins}

%\usepackage{chemformula}
%\usepackage{setspace}
%\usepackage{yfonts,color}
%\usepackage{siunitx}
%\usepackage{comment}
%\usepackage{multirow}
%\usepackage{varioref}
%\usepackage[bottom]{footmisc}
%\usepackage{wrapfig}
%\usepackage{float}
%\usepackage{type1cm}
\usepackage{lettrine}
\linespread{0.9}
%\usepackage{chngcntr}
\usepackage[nottoc, notlof, notlot]{tocbibind}
%\onehalfspacing
%\counterwithout{footnote}{chapter}

 
\title{Continual Learning for Activity Recognition using hybrid synthetic-real datasets}
\author{Flavio Lorenzi}
\IDnumber{1662963}
\course[]{Artificial Intelligence and Robotics}
\courseorganizer{Department of Computer, Control and Management Engineering Antonio Ruberti}
\submitdate{2020/2021}
\copyyear{2021}

\advisor{Prof. Luca Iocchi (Sapienza)}
\advisor{Francesco Puja (Konica Minolta)}

\authoremail{lorenzi.1662963@studenti.uniroma1.it}
\examdate{July 2021}
\examiner{Prof. ...} \examiner{Prof. ...} \examiner{Prof. ...}  \examiner{Prof. ...}  \examiner{Prof. ...} \examiner{Prof. ...}  

%we refer to http://ctan.mirrorcatalogs.com/macros/latex/contrib/sapthesis/sapthesis-doc.pdf for an exhaustive description of the sapthesis documentclass.


\begin{document}

\frontmatter
\maketitle

\begin{abstract}
Nowadays with the development of deep learning and computer vision techniques, Human Activity Recognition is increasingly becoming a key task and much requested in real scenarios, like in the field of safety and security; for instance cameras on a construction site which can detect if a worker is injured, a bank alarm system for dangerous situations or smart cameras that, in Covid-19 times, might be able to perform a cough detection simply from an individual's gestures. There are many ways to treat it in the State Of The Art. In this work I first show and implement the standard canons of Deep Learning for HAR, then I break away from them by taking into account an innovative methodology, called Continuous Learning, with the aim to overcoming the well known limitations of the classical approaches. In particular the Elastic Weight Consolidation (EWC) approach has been replicated and tested on a Convolutional Neural Network, trying to exploit as many neurons as possible to train the model to recognize new activity classes without forgetting the previous, avoiding the recurrent problem of \emph{"Catastrophic Forgetting"}. Very interesting results are achieved using a custom synthetic dataset generated from scratch with Unity platform and then predict with the trained model over two different real datasets. In this work all these aspects are analyzed and developed, observing deeply the concept of Activity Recognition from different points of view, making several comparisons between the built models and the results obtained from experiments.

\end{abstract}


\makenomenclature

\mbox{}

\nomenclature{$AR$}{Augmented Reality}
\nomenclature{$ANN$}{Artificial Neural Network}
\nomenclature{$API$}{Application Programming Interface}
\nomenclature{$CNN$}{Convolutional Neural Network}
\nomenclature{$CL$}{Continual Learning}
\nomenclature{$CPU$}{Central Processing Unit}
\nomenclature{$CUDA$}{Compute Unified Device Architecture}
\nomenclature{$CV$}{Computer Vision}
\nomenclature{$DL$}{Deep Learning}
\nomenclature{$DoF$}{Degree of Freedom}
\nomenclature{$GPU$}{Graphical Processing Unit}
\nomenclature{$HAR$}{Human Activity Recognition}
\nomenclature{$EWC$}{Elastic Weight Consolidation}
\nomenclature{$KM$}{Konica Minolta}
\nomenclature{$LSTM$}{Long Short-Term Memoryk}
\nomenclature{$ML$}{Machine Learning}
\nomenclature{$RNN$}{Recurrent Neural Network}
\nomenclature{$POV$}{Point Of View}
\nomenclature{$R&D$}{Research & Development}
\nomenclature{$ROG$}{Republic Of Gamers (Asus)}
\nomenclature{$SOTA$}{State Of The Art}
\nomenclature{$SMPL$}{Skinned Multi-Person Linear model}
\nomenclature{$VR$}{Virtual Reality}
\nomenclature{$YOLO$}{You Only Look Once}

\printnomenclature


\thispagestyle{empty}
\listoffigures
\newpage
\pagenumbering{arabic}


\tableofcontents

\mainmatter
\chapter{Introduction}
\lettrine[lines=2, findent=3pt, nindent=0pt]{H}{}uman Activity Recognition (HAR) is a wonderful challenging task which consists in classification of specific movements or actions of people identified by sensor data. HAR approaches have made huge progress in the last years, accomplishing several ways to exploit it, obviously according to the current input data type. 
A very interesting thing is the achievement of important results in this field thanks to the introduction of an innovative approach, the Continual Learning, which moves away from the standard deep learning canons in many aspects, overcoming its several limits.
Following it is shown the work I carried out during my six months of external thesis internship at Konica Minolta company, in a R&D team aimed to the realization of machine learning and computer vision projects, using the “Agile Software Development” methodology for assigning tasks and goals. In particular the aim of my thesis work was to perform the Activity Recognition engine, implementing a Continual Learning methodology which allows me to achieve exellent results in this field. So thanks to this opportunity, not only I have gained a deep experience in the AI fields of Machine Learning, Computer Vision and Computer Graphics, but I also actively participated in my team meetings and company activities, learning the organizational methodology used, coming in contact with this stimulating world.
\section{Problem statement and motivations}
The aim of HAR is to analyze and detect human activities (Figure 1.1) from the information acquired from sensors such a sequence of images, either captured by RGB cameras (Kinect, webcam…), range sensors, or other sensing modalities. Most of the research work mainly focuses on a spatial and temporal aspect of a video sequence of action recognition captured by RGB cameras. However, the image may suffer from illumination, various points of view, self-occlusion problems, clutter background and body segmentation error. These are the issues of color videos which influence the accuracy performance, especially when dealing with complex human actions.
Furthermore, a big problem we encounter is lack of datasets in circulation: today this affects the entire world of the deep learning in general. Obviously, there are the state-of-the-art datasets used for these porpouses, but not all of them are available for free, or the number of samples/activities used could be limited. 
\begin{figure}
\centering
\includegraphics{Images/HAR.png}
\caption{Generic Human Activity Recognition engine}
\end{figure} 
In this elaborate, all these problems are overcome by the fact that the original training dataset is synthetic and completely generated from scratch thanks to Unity development platform: in this way we can capture noise-free frames of any activity we want. Moreover the performed AR engine is based on the human skeleton joints extraction avoiding in this way most of the problems mentioned before. This specific technique is very strong because it allows us to keep track of every movement of the human body through the extraction of 3D points related to the limbs, torso, head of the skeleton: if this is done for each frame of a specific sequence that determines an activity, it is possible to analyze the latter in different ways by introducing deep learning techniques, both by keeping track of the temporal-motion of these joints (i.e. with RNN, LSTM ...) and by observing the 3D points as possible barcodes and then performing a simple classification between images (i.e. with Convolutional Neural Networks). \\ \\
In the field of artificial intelligence it is important to try to have a constant passion for research, innovation and improvement of what you already have: this is precisely the reason that prompted me to go beyond the classic learning techniques described above, introducing an innovative technique, which aims to constantly improve the classic techniques and tries to go beyond the many limitations they present. All this is enclosed in the concept of Continual Learning, which allows a model to learn new informations without having to re-learn everything from scratch and above all without forgetting what it already knows: this technique is very useful for example in projects that starts with a limited number of classes / activities to be classified and then over time this is increased exponentially, for example following business trend and demand, in the case of companies, or just because at start not the entire dataset is available yet, perhaps in progress with advanced tools of synthetic dataset generation such as Unity, NVIDIA Omniverse and so on.
HAR’s potential benefits from Continual Learning are obvious: humans dynamically change their behavior and even develop new activities. Hence, algorithms must adapt to such ever changing diverse behaviors to prevent service quality degradation.

\section{Related works}
\textbf{3D pose estimation for HAR skeleton estimation.} Many documents formulate human pose estimation as the problem of locating the major 3D joints of the body from an image, video sequence, single or multiple view. We argue that this notion of \emph{"pose"} is overly simplistic, but it is the main paradigm in the field. The approaches are divided into two categories: two-stage estimation and direct estimation. Two-step methods first predict 2D joint positions using 2D pose detectors or 2D groundtruth position and then predict 3D joint positions from 2D joints by regression or model fit, where a common approach leverages a learned dictionary of 3D skeletons. Most methods assume the length or proportions of the limbs. Two-step methods have the advantage of being more robust to domain displacement, but rely too heavily on 2D joint surveys and can eliminate image information in 3D pose estimation. Groundtruth motion capture video datasets define the problem in terms of 3D joint positions. They provide training data that allow the 3D conjoint estimation problem to be formulated as a standard supervised learning problem. Therefore, many recent methods estimate 3D joints directly from images in a deep learning framework. The dominant approaches are completely done with convolutional neural networks, with the exception of a few more recent methods of regressing the bones, obtaining excellent results on the 3D pose benchmarks. Many methods do not fix the camera, but they estimate the depth relative to the root and use a predefined global scale based on the average length of the bones. The main problem with these direct estimation methods is that accurate three-dimensional annotated images of the terrain are captured in controlled MoCap environments. Models trained on these images alone don't generalize well to the real world.\\\\
\textbf{LSTM techniques for HAR time series classification.} LSTM network models are a type of recurrent neural network that are able to learn and remember over long sequences of input data. They are intended for use with data that is comprised of long sequences of data, up to 200 to 400 time steps. So they are very good to fit the Human Activity Recognition problem. This model can support multiple parallel sequences of input data, such as each axis of an accelerometer and gyroscope data. The model learns to extract features from sequences of observations and how to map the internal features to different activity types.
The benefit of using LSTMs for sequence classification is that they can learn from the raw time series data directly, and in turn do not require domain expertise to manually engineer input features. This model can learn an internal representation of the time series data and ideally achieve comparable performance to models fit on a version of the dataset with engineered features. With this approach a several number of SOTA optimal results has been achieved: for example it was proposed a kind of differential recurrent neural network which emphasizes on the change in information gain caused by the salient motions between the successive frames; a parallel approach instead provides a kind of regularized deep LSTM network which take the skeleton as the input at each time slot and introduce a novel regularization scheme to learn the co-occurrence features of skeleton joints; furthermore a more powerful tree-structure based traversal method is implemented, to handle the noise and occlusion in 3D skeleton data, they introduce new gating mechanism within LSTM to learn the reliability of the sequential input data.
Although the methods of Dynamic Time Warping, Fourier Temporal Pyramid and Hidden Markov Models are useful in temporal dynamics problems, the recent use of LSTM networks has shown superior performance for modeling temporal dynamics compared to traditional methods. Since all of this research has generally only looked at the long-term memory of human actions, it can be difficult to fully model various temporal dynamics including short/medium-term actions and so on. \\ \\
\textbf{CNN techniques for HAR.} Convolutional Neural Networks (CNNs) have also been applied to this problem. A very interesting approach proposed the Joint Trajectory Maps (JTM), which represents both spatial configuration and dynamics of joint trajectories into three texture images through color encoding, and then fed these texture images to CNNs for classification. Another methodology proposed instead to represent each skeleton sequence as an image, where the temporal dynamics of the sequence are encoded as changes in columns and the spatial structure of each frame is represented as column .How to effectively represent 3D skeleton data and feed into deep CNNs is still an open problem.\\ \\
\textbf{Continual Learning approaches in HAR.} Regarding this unconventional learning mechanism, there are not too many state-of-the-art works that exhibit good performance and results in the field of HAR, due to the fact that most regularization approaches lack substantial effects and provide insight, when they fail. Therefore the development of continuous learning algorithms should be motivated by quite different fields of activity. CL algorithms that take advantage of regularization attempt to alleviate oblivion by limiting updates on network parameters: a substantial amount of these have made significant progress on image datasets, it is important to verify their generalization abilities to other domains such as HAR which is marked by important factors like 1) dataset unbalance where the frequencies of activities can vary a lot and some are recurrent while others are rare, 2) similarity between classes: activities could resemble each other thus forming overlapping boundaries between classes; 3) diversity within the class: an activity can be carried out in different ways; 4) resource constraints: most HAR systems are deployed on devices with memory and compute constraints such as wearables. Therefore, in numerous approaches it has been found that regularization terms can often lead to little or even detrimental effect on HAR scenarios.  \\ \\
In this paper all these problems will be analyzed in detail. In particular the Continual Learning “Elastic Weight Consolidation” approach is implemented for the proposed Convolutional Neural Network, overcoming any encountered problem with a very accurate tuning of the network parameters and achieving good results in this complex field.



\chapter{Unity: a cutting-edge platform for dataset generation}
\label{chap:1} 
\lettrine[lines=2, findent=3pt, nindent=0pt]{U}{}nity is the most popular cross-platform game engine that packs a lot of features together and is flexible enough to create almost any game scene you can imagine; this advanced tool was born with the aim of “democratizing video game development” and making the development of interactive 2D and 3D content as accessible as possible to all people. Thanks to a well-made graphical interface and the possibility to work with C\# coding to manage each internal game object, it is very intuitive to create realistic scenes depicting everything we want (Figure 2.1). 
So this real-time 3D development platform consists of a render and physics engine, as well as a graphical user interface called Unity Editor; it has received wide adoption in the gaming, AEC (Architecture, Engineering, Construction), automotive and film industries and is used by a large community of game developers to run a variety of interactive simulations, ranging from small mobile games and browser-based games to high budget console games and Augmented / Virtual Reality experiences. All this constitutes an almost unequaled strength, which makes it today, despite the various alternatives born in recent years, one of the most powerful tools in the field of computer graphics and beyond.
\begin{figure}
\centering
\includegraphics{Images/env_unity.png}
\caption{Realistic environments generated with Unity}
\end{figure} 
Unity's historical focus on developing a generic engine to support a variety of platforms, developer experience levels, and game types makes the Unity engine an ideal candidate simulation platform for AI research. The flexibility of this engine allows for the creation of activities ranging from simple 2D grid-world problems to complex 3D strategy games, physics-based puzzles or possible multi-agent competitive games. Unlike many of the existing research platforms, this engine isn't limited to any specific genre of game or simulation, making Unity a general platform. In addition, Unity Editor enables rapid prototyping and development of simulated games and environments. 
This platform is very appreciated from machine learning engineers, in particular for a very specific task: the synthetic dataset generation. It is well known that one of the greatest limitations of deep learning world is the lack of data on which train and test models; in any case there are many available dataset for everyone and made free for research, such as ImageNet (containing hundreds of categories of images depicting objects, people…) or MNIST (containing more than 50k of handwritten digits); but it is still evident that is not enough. The solution lies in making a switch from the real domain to the synthetic one, which thanks to tools like Unity that make it realistic, can be exploited for the same purposes as deep learning, maybe taking into account a possible loss in terms of accuracy, but it is clear that the pros are greater in comparison: it is enough to have basic skills in the world of computer graphics and C-based programming languages to create any scene you want. Many sites such as \emph{cgtrader}, \emph{free3D}, \emph{turbosquid}, \emph{mixamo} (and so on) allow you to download free basic models (advanced models have to be paid for) depicting objects, people, animals, with raw materials and textures available too; moreover predefined animations can be downloaded together with these characters and customized afterwards as you want.
In this chapter we will see step by step the external tools that allows dataset generation, starting from the general description of Unity Simulator (aimed to Computer Vision) up to describe how the simulated scenarios have been built with custom human activities, that will be the main object of our study case.

\section{Preliminaries}
A Unity project consists of a collection of Assets (resources). These typically correspond to the files within the project. Scenes are a special type of resource that defines the environment or level of a project. Scenes contain a definition of a hierarchical composition of GameObjects, which correspond to real objects (physical or purely logical) within the environment. The behavior and function of each GameObject are determined by the components connected to it. There are a variety of built-in components that come with Unity Editor, including cameras, meshes, renderers, RigidBodies, and many more. As mentioned before it is possible to define custom components too using C\# scripts or external plug-ins. Follow a brief overview of main Unity components and internal mechanisms is described, reporting only the essential features which alllow the operation of this advanced simulator. \\ \\
The Unity GUI is characterized by several panels, each one with a specific function:
\begin{itemize}
    \item The “Hierarchy” is the main scene manager which allows to control properties about cameras, lights and each GameObject (every object in the scene, like humans, cars, animals…), together with the help of another control panel called “Inspector”: the first is used for object components checking while the second for properties modifying. Moreover in the Hierarchy the simulation-scenario tool plays a key role managing scene frames number and iterations for each frame.
    \item The  “Project” panel is the main folder which contains the Assets folders (set of fbx samples, animations, textures, materials and scripts): this is used if we want to import new GameObjects or just check the preliminary models and material that Unity makes available (called Prefabs).
    \item The “Console” is helpful to see system messages (errors, warnings, text prints…) and the “Animator” panel to manage the animations.
    \item “Scene” and “Game” panels are respectively used to control the objects in the space (pick, rotate, …) and see the final result, given from combination of all the parameters set by these panels described so far; to start the simulation, just press the RUN button.
\end{itemize}
All these graphic panels constitute the Unity engine and they can be used by the user in a coordinated and joint way, in such a way as to create any type of simulation he needs.
\begin{figure}
\centering
\includegraphics[scale=0.215]{Images/custom_unity_interface.jpg}
\caption{Unity custom GUI containing all described panels}
\end{figure} 


\section{Perception tool: Unity for Computer Vision}
Today there are many external toolkits aimed to deep learning that can be viewed as third party libraries, due to Unity ability to be  fully compatible and scalable with new inputs. Fortunately there is big set of toolkits completely dedicated to the Computer Vision world (Figure 2.3): it is possible customize the method of labeling that your application requires, from simple bounding boxes to complex semantic annotations, impossible to obtain through the classical manual labeling.\\
In Unity Computer Vision Datasets projects, everything about the environment can be randomized to create diversity in your dataset. Lighting, textures, camera position, lens properties, signal noise, and more are all available for randomization to ensure that your dataset covers the breadth of your use cases. With synthetic data, the environment that provides the context for the computer vision problem may not necessarily resemble a real-world environment. Datasets for some computer vision tasks may simply require a highly randomized background, whereas others may demand more structure, such as a building or home interior.

\begin{figure}[ht]
\centering
\includegraphics{Images/unity_cv.png}
\caption{Unity Computer Vision labeling toolkits}
\end{figure} \\ 
After a brief scouting in Computer Vision available tools for Unity, I found the Perception package; it provides a toolkit for generating large-scale datasets for computer vision training and validation. It is focused on a handful of camera-based use cases for now and will ultimately expand to other forms of sensors and machine learning tasks. Perception offers a variety of tools for generating synthetic datasets intended for use in perception-based machine learning tasks, such as object detection, semantic segmentation, and so on. These datasets are in the form of frames captured using simulated sensors. These frames are annotated with ground-truth and are thus ready to be used for training and validating machine learning models. While the type of ground-truth bundled with this data will depend on your intended machine learning task, the Perception package already comes with a number of common ground-truth labelers which will make it easier for you to generate synthetic data. \\
Following the proposed tutorial it was possible to configure Unity from scratch on the company laptop (ROG with Geforce RTX and Ubuntu 20.04 LTS), up to the generation of a large-scale synthetic dataset for training an object detection model. While this process may seem complicated, you don't need to have any previous experience with Unity or C\# to follow the first step of this tutorial, starting to generate datasets using the supplied samples (prefabs) and components. \\
Once the first setup was performed (Figure 2.4) we are able to generate a labeled dataset characterized by randomly objects, very useful for CV and ML tasks like image classification or object detection.
However, since the goal of this section was to generate a dataset for the HAR, people and animated subjects are needed in the new dataset: so I switched to the \emph{Human Pose Labeling and Randomization} tutorial which contains step by step instructions for using the keypoint, pose and animation randomization tools included in the Perception package. This section is very useful not only to learn how to generate labelled dataset but it contains guidelines for import and customization of models and animations. \\
It is organized in this way:
\begin{itemize}
    \item how to import fbx models and animations
    \item set up humanoids characters in the scene
    \item set up perception camera for key-point annotations
    \item set up labels and key-point templates
    \item randomize the scene
\end{itemize}
So following this is possible to obtain basic humanoids with no attached textures (Figure 2.5) able to perform predefined animations and movements in the scene; getting to this type of output is very useful to understand the correct functioning of each control panel in the graphical interface and understand what are the tools not only to test ready-made things, but also to create them from scratch. Furthermore, this tutorial allows you to assimilate the correct functioning and setting of the labels for each human-character (both bounding boxes and skeleton joints).
Now running the simulator we obtain for each scene the following result: a dataset folder characterized by 1) specified rgb frames, 2) data captures with human labels.
\begin{figure}
\centering
\includegraphics[scale=0.26]{Images/perception_guide.png}
\caption{Perception tutorial phase one: clarity and rigor of the explanations}
\end{figure} 
\begin{figure}[!b]
\centering
\includegraphics[scale=0.12]{Images/human_pose_unity.png}
\caption{Perception - Human Pose Estimation: some output example}
\end{figure} 




\section{Custom dataset generation}
In this section it is explained how I have built up and organized the scenes which contribuited to the generation of my HAR dataset.
Starting from Perception tutorial final output, all the prefabricated humanoids were deleted, just keeping the general configuration setup: in particular SimulationScenario, Directional Lights and Main Camera settings; in particular I used the Perception Camera script (got by tutorial), very useful to keep labels enabled and to obtain the dataset in the same output format I specified in the previous section.
The first thing was the import of each needed fbx model, animation, texture… and with a little immagination I made my first raw scenes (Figure 2.6). Step by step I imported new materials getting closer to my goal.
To make all objects easily controllable I wrote a C# script associated to an invisible gameObject (called Configuration): this allowed me to create a link with any other object, just with the command $GameObject.Find(gameobject\_name)$. 
Furthermore this script estabilishes a randomization process for cameras, textures, lights, objects (cars, buildings, trees), humans with related animations and animals too. In this way we will always obtain different outputs: in CV this falls into the field of Data Augmentation and it is obviously useful for increasing the amount of information we are going to work with.\\
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{Images/mixamo.png}
\caption{Only a partial collection of the thousands of free models with related animations, available in mixamo.com}
\end{figure}
\subsection{Scene making}
Following we will see how the scenes were made from scratch, importing models and randomizing them in order to obtain a big amount of data to collect for our custom dataset generation.\\\\
The generated activities are:
\begin{itemize}
    \item Waving Hands  (WH)
    \item Hands Up  (HU)
    \item Falling Down  (FD)
\end{itemize}
Waving hands is a non-verbal communication gesture that consists of the movement of the hands and / or the entire arms that people commonly use to greet each other, but it can also be used to get noticed, recognize someone else presence, ask for help or deny something to someone.\\
Hands up is another non-verbal communication gesture, similar to waving, but which consists in raising both hands to generate a request, generally a question but which can actually turn into a surrender gesture depending on the context.\\
Falling down is the activity that develops as a consequence of a loss of balance that can be due to various reasons, such as slipping or being hit. \\
The main reason of this choice is due to the topic of the current company R&D project in which I was included in Konica Minolta: "$Safety$ \& $Security$". In fact, an activity recognition that manages to classify these three activities becomes an excellent weapon in possible vision and alarm systems. \\
So we will deal with three possible scenes (one for each activity), well implemented with a joint combination between the code and the engine panels: for example each one can be switched thanks to the variable (int) $select\_scene$ in the code, while different fbx models (humanoid) are imported and managed in the Assets panel.
Animations are all randomized, minimizing repetition of identical movements and redundant data: all this was obviously possible thanks to the implementation of the support code, in which variables such as speed of movements, orientation ... have been well randomized with the Unity $ Random.Range (a, b) $ command that is able to take a decimal number in the range between $a$ and $b$ each time. However the Graphic Interface is important too: indeed thanks to the "$Animator$" control panel, this one allows you to modify each animation of the characters at will, in terms of limb and torso angle.\\
The rest of the scene randomization is performed by 1) cameras 2) objects (even if not so useful for activity recognition tasks).\\ \\
- Cameras are organized in this way: four different points of view (POV) (Front, Right, Left and Back views) always facing the character performing the current activity. Each one is randomized in turn with a random picking of numbers which update for every new scene the position and the orientation coordinates $(x, y, z)$. In this way data augmentation technique (zoom, rotation...) are performed. For example the position coordinates are randomized in this way: \\ $camera.transform.position = new Vector3(rnd\_posX, rnd\_posY, 0f)$ \\ where $rnd\_posX$ and $rnd\_posY $ are float random numbers, generated by the \\ $Random.Range(a,b)$ function. Therefore the intersection of all these random variables makes the generated data almost always different from each other, guaranteeing a slight difference between every scene of our simulated environment (Figure 2.7).
\\- The background is organized in a very similar way, managing several random picking for near objects like for instance cars (colors and position/orientation), buildings (height and rotation), animals (animations), directional light (on/off) and outline characters (animations and weapons). This last thing in particular has been done in an exhaustive way, not so much for the activity recognition task, but to have a starting dataset possibly reusable with future tasks: indeed several weapons are handled by some outline character. For example, in \emph{hands up} scene a guy holding a baseball bat may appear close to people carrying out the activity. \\ In \emph{waving hands} several guys can alternate between knives and guns bats in their hands. In \emph{falling down} there's a guy with a machine gun shooting wildly (Figure 2.8). Thanks to these additional implementations, it will therefore be possible to create weapon detection tasks for possible dangerous situation diagnosis.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.16]{Images/waving.jpg}
\caption{Randomization example of waving hands activity for first and last frames}
\end{figure} \\
Finally we need to activate the labels we need: to perform activity recognition we need 1) bounding boxes of the character carrying out the activity, 2) the joints of the skeleton and how they change over time. The Perception package explain step by step how to do quickly all this, with the keypoint labeler external tools: it allows you to easily activate the labels we need from the graphic interface by specifying the names of the activities to distinguish them. In this case, the skeletons joints are captured even if, as we will see in the next chapters, the skeleton estimator is an external third part that plays a key role for universal (not only Unity-made) datasets.

\begin{figure}
\centering
\includegraphics[scale=0.6]{Images/mandown_badguy.png}
\caption{\emph{"Back View"} in falling down activity; we can see the bad guy holding baseball bat and a man with a gun too}
\end{figure}

\subsection{Dataset collection}
Once the scene are complete and guarantee an exhaustive randomization factor, we just have to press the RUN button and generate the dataset. Very easy right? Unfortunately not in this case: unlike classic ML datasets where each collected frame is equivalent then to a single sample (as it happens in classic object detection tasks), in activity recognition tasks each sample contains in general a sequence of frames that determine the action. The number of frame obviously depends on the activity duration, but this is not binding if you use parametric deep learning techniques to interpret them. So in this case I decided to fix a larger sequence a priori equal to 250 (and to sample it later during the dataset preprocessing).\\ \\
The main problem here is that Unity Perception Tool is thought to allows the creation of single-frame dataset (as described first). So by pressing the RUN button, it will perform a single random scene, collect the dataset (250 frame and related bounding boxes labels) and then stop. This means you are forced to click RUN each time by hand for each sample, which for a dataset consisting of thousands of samples is difficult to do (and also goes against all the principles of IT engineers, who always aim to automate all processes). \\
Scouting possible solutions I found the following tool for Linux: $xdotool$; this allowed me to automate the mouse movement and click just by assigning to it the button location in the screen (x and y coordinates), number of clicks, relative delay between each one and the chosen between right and left click button (C parameter). This was performed just running the terminal command: \\
\$ xdotool mousemove x y click –repeat “number” –delay “millisecond”s C
\\ In my case, I chosen the following configuration: -number = 500; -milliseconds (per scene) = 16000; -x = 942; -y = 106; -C = 1. \\
So I collected a total of 500 samples for each of the 4 cameras and for each of the 3 activities, achieving a total of 500 x 4 x 3 = 6k samples.\\ \\
During this phase several problems have arisen:
\begin{itemize}
    \item large space requirements
    \item long waiting times
    \item inadequate system-optimization for Unity
\end{itemize}
The first problem was about the large disk-space required to perform the collection (each collected activity folder is about 220 gb); moreover during the preliminar phases of dataset parsing, as we will see in the next chapters, the data will become greater too: so a huge disk-space is needed.
The second problems is related to the first: a big number of frame per sample requires very long dataset generation times. If the Unity simulation takes 16 seconds to collect each sample, so we will have 32k seconds for each activity, so a total amount of time equal to 26,6 hours for the entire dataset generation.
Another faced problem was the third announced above: Unity simulations not always worked fine and sometimes got stuck because Ubuntu system is not optimized for this simulator. So a python script was needed, checking if generated frames are all (250) or not for each sample; if there are incomplete samples, this script removes them and tells us to recollect them.



\section{Unity potential: single-frame dataset generation for man down detection}
This section concerns the potential of Unity engine in the field of ML and CV: in particular we will see all its strength when aimed to single-frame data generation. \\ As I mentioned before, ML-oriented Unity tools (like the Perception package) are oriented to quick generation of frames that will become part of a synthetic dataset, each one as different sample of it. So once randomized scenes are complete we just have to specify, in the Perception camera panel, the number of total frames we need and then click RUN: unlike as seen for activity recognition, in this case it is possible to generate the entire dataset in a very quick and easy way. All the problems described at the end of the previous section disappear in this context (short waiting times and very light dataset in terms of space).\\ \\
I was able to test these potentials thanks to a parallel task carried out during the KM internship: training of a neural network on a single-frame dataset generated with Unity, concerning a binary classification (detection) of two classes of activity: man down and standing person (Figure 2.9). The aim of this work is related to the topic currently addressed by my R&D team (Safety \& Security). They quickly needed such a synthetic-trained model to test on a real domain dataset.
\\
So, starting from the work I already done with Unity, it was easy remove animations (single frames required: men standing or on the ground) and adapt all the code according to my needs. In particular I: (1) changed the way my camera captures the scene, dragging it through an imaginary circle around the characters and randomizing their height each time; (2) shifted in the main loop of the code all update variables such that for every iteration, each generated frame was randomized, changing cameras and characters avoiding repetitions of similar data (data augmentation); (3) removed all redundant outline objects or character.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.41]{Images/mandown_det.jpg}
\caption{\emph{"Front view"} of two different samples for standing and man-down}
\end{figure}
\\
The dataset preprocessing (data parsing and skeleton joints extraction) was implemented in the same way as it was done for the activity dataset, which makes this whole procedure parametric and totally scalable.
Since the objective of this thesis does not concern single-frame preprocessing for \emph{man-down detection} and the final accuracy I retrieved with this particular type of dataset, I will not show how all this was accomplished, nor the results I achieved with the custom 1D CNN I implemented. In any case, excellent results have been achieved with a very simple network, reaching a very high accuracy on the synthetic domain and (with a small loss) on the real domain too (Figure 2.10). \\ 
These results make us understand how, without the use of human forces to generate real images to process and without having to manually label the objects of interest, with Unity Simulator, it is possible to create entire datasets in a very short time which in most cases (or at least for the first training) can be completely substituted to the real ones to train neural networks - deep learning models. Obviously this is a turning point in the fields of ML and CV, because it saves costs, time and resources.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.24]{Images/real-standing.png}
\includegraphics[scale=0.24]{Images/real-mandown.png}
\caption{On-line prediction for man-down detection in a real scenario, performed by KM team with the model I trained over synthetic domain}
\end{figure}



\chapter{Overcoming deep learning boundaries with  
Continual Learning}
\label{chap:2}
\lettrine[lines=2, findent=3pt, nindent=0pt]{A}{}ctivity recognition is widely recognized as a central topic in computer vision, as evidenced by the enormous amount of research done in recent years; it has evolved from the previous focus on action and gesture recognition. The main difference is that activity recognition is completely general, as it covers any kind of human activity, which can last a few seconds or minutes or hours, from everyday activities such as cooking, taking care of oneself, talking on the phone, cleaning a room, to sports or recreational activities such as playing basketball or fishing. 
Thus, movement interpretation is essential to ensure recognition and classification of activities. Just think of sports activities, or cooking, or performing arts, which require intentionally selecting a specific sequence of movements. Similarly, everyday activities such as cleaning, or cooking, or washing dishes, or setting the table require precise sequences of movement to accomplish the task. Indeed, the compositional nature of human activities under bodily and kinematic constraints has attracted the interest of many research areas such as computer vision, neurophysiology, sports and rehabilitation, biomechanics, and robotics.
The goal of activity recognition is therefore to be able to identify such mechanisms. At the state of the art there are hundreds of different ways to do this, some better than others.\\ \\
In this chapter I'm going to list the main steps which have allowed to carry out properly the HAR task, given the synthetic dataset, previously generated with Unity simulator. The first part provides a generic overview of deep learning and the world of neural networks. The second section talks about the pros and cons of the classical learning techniques employed today to face off this important topic (HAR), comparing mainly two types of networks: Convolutional Neural Networks and Long Short-Time Memory networks. The next section explains how the key data that these types of networks need are extracted from the dataset used and the techniques that enabled their preprocessing. Finally, the last section highlights all the featured problems in the classic deep learning approaches both in general and in this specific area, introducing the Continual Learning (in particular the EWC technique) as a possible solution to these difficulties: in particular, there will be details about the theoretical aspects on which this is based on, because since it is a relatively new approach, it is important to understand CL thoroughly in order to use it.

\section{Deep learning preliminaries}
Artificial Neural Networks (ANNs) arise from the idea of emulating the functioning of the human brain. These network systems are based on the resolution of a given task, in particular the process derives from a series of mechanisms to learn the various information available in order to deduce new information, gradually improving the performance of the machine itself. They are now indispensable structures to solve engineering problems of artificial intelligence and they also work in different technological fields such as electronics, computer science, simulation and other disciplines; considering computer vision for example, they can be used to recognize particular patterns within an image, i.e. it is possible to distinguish objects in different images from images labeled with object{"present or missing object"}. The most important thing is to analyze the mechanism by which these neural networks are adaptive with respect to the external information that determines the learning outcome. It is based on the concept of recreating the behavior of a single neuron in the human brain through an artificial model that connects each neuron with the next through synapses. A synapse (Figure 3.1) is a connection dedicated to the transmission of an electrical signal between neurons. When the impulse reaches the terminal of the synapse, a stack of neurotransmitters is released. Synapses can learn from the stimulation given by the signal passing through them. This history dependence acts as a memory system that mimics human memory.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.45]{Images/syn.png}
\caption{Complex structure consisting of a neuron plus synapses}
\end{figure}\\
Let's go back to the artificial concept on Neural Networks; given an input signal, this model produces output processing influenced by an appropriate value called \emph{weight} which is essentially a threshold value for the inputs. All of these \emph{"neurons"} are linked together to quantify the value of the input in order to allow the entire network preferential paths depending on the available weight. The most widely used architectures in practical applications are the \emph{"Feedforward networks"}, a simpler type of ANN in which the connections between nodes do not have loops or loops, so the data flow through the network in one direction only (forward) starting from the input nodes to the output nodes. In the case where each neuron receives input from the output of each neuron in the previous layer, the network is called a multilayer perceptron neural network, i.e. MLP NN. The typical scheme of a \emph{"Feedforward MLP neural network"} is shown in Figure 3.2: on the left, the input is handled and prepared to be sent to the mixture neurons; in the middle the part dedicated to the actual data processing, called \emph{"black box"} because of the hidden layer usually composed of thousands of neurons (with hundreds of thousands of interconnections); the final layer, visible on the right, has the function to adapt as output the results obtained from the network processing.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.2]{Images/nn_classic.png}
\caption{MLP Feedforward NN structure.\\ On left: input layer that containing the input data; in the middle: the hidden part of the network devoted to processing the data; on the right: the output layers as the adaptation of the results obtained.}
\end{figure}
\subsection{The perceptron, Delta rule and Supervised Learning}
The Perceptron is an artificial model born with the intent to reproduce the typical behavior of biological neuron (which is still not completely clear). This model is composed of three crucial elements such as: connecting links composed by a synaptic weights, a simple linear combiner in order to sum the input signals, and finally an activation function AF for the attenuation of the amplitude of the output neuron. The output provided by the neuron is essentially a non-linear function of linear combinations of inputs and synaptic weights summed with an external threshold $\theta \in R$, also called bias, as:
\begin{equation}
y = \phi (\sum_{j=1}^{M} w_j x_j + \theta)
\end{equation} 
in case of an additional input $x_0$ such that $x_0 = 1$, the weight $w_0$ is equal to the bias $\theta$ and the Equation 3.1 becomes:
\begin{equation}
y = \phi (\sum_{j=0}^{M} w_j x_j)
\end{equation} 

The activation function has biological characteristics, in particular if the sum of the weights, given an input stimulus, is greater than the bias then the output is high, otherwise it is low:
\begin{equation}
\begin{aligned}
\sum_{j=1}^{M} w_j x_j + w_0 \ge 0 \rightarrow y = high \\
\sum_{j=1}^{M} w_j x_j + w_0 < 0 \rightarrow y = low
\end{aligned}
\end{equation} 

\begin{figure}[ht]
\centering
\includegraphics[scale=0.53]{Images/perceptron.png}
\caption{The perceptron structure}
\end{figure}

This simple hard limiter represents the biological behaviour of the cell potential as the output of the linear combiner (Figure 3.3):
\begin{equation}
s = w^T x
\end{equation} 
with $w$ $\in R^{(M+1)\times1}, x \in R^{(M+1)\times1}$ are the arrays defined as:
\begin{equation}
\begin{aligned}
x=[1, x_1, \dots, x_M]^T\\
w=[w_0, w_1,\dots ,w_M ]^T
\end{aligned}
\end{equation} 
Let us focus now on the procedure for updating the weights of the inputs to a MP neuron, the Delta-Rule. This algorithm is a gradient descent learning rule, a fundamental part used in many learning approach of the neural networks.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.48]{Images/gdr.png}
\caption{Generalized Data Rule (GDR)}
\end{figure}

Let us consider the following adaption rule as:
\begin{equation}
w_n = w_{n-1} + \frac{1}{2} \mu [- \nabla \hat{J}(w)]
\end{equation}

in which $\hat{J}(w) \hat{=} e[n]^2$ with $e[n]$ a priori error at time $n$ as $e[n] = d[n] - \varphi (w_{n-1}^T x[n])$. By exploiting the presence of the non-linear function we 
can rewrite the formula of the gradient through the following mathematical steps: 

\begin{equation}
\begin{aligned}
\nabla \hat{J}(w) &= \frac{\partial \hat{J}(w)}{\partial e} \cdot \frac{\partial e}{\partial s} \cdot \frac{\partial s}{\partial w_{n-1}} \\
&= \frac{\partial e^2}{\partial e} \cdot \frac{\partial [d-\varphi (s)]}{\partial s} \cdot \frac{\partial (d - w_{n-1}^T x)}{\partial w_{n-1}} \\
&= -2e \cdot \frac{\partial \varphi (s)}{\partial s} \cdot x
\end{aligned}
\end{equation}

Once you have inserted the Equation 3.7 inside the Equation 3.6 of the adaption rule, you obtain the final equation of the GDR for MP-neuron.

A brief final parenthesis now is to explain what is meant when we talk about \emph{Supervised learning} (or \emph{Learning with a teacher}); in this context we refer the machine learning task of learning a function that, given examples of correlations between input and output, performs a map. This paradigm of learning modifies the synaptic weights of a neural network by applying a set of data called training set, called also training samples. The whole learning procedure is performed until there is minimal variation between the synaptic weights.

\subsection{The back-propagation learning algorithm}
The back-propagation algorithm allows the correction of the error output $y$ as $e=d-y$, where $d$ is the desired signal. In this way the connection weigths inside the networks gradually adapt this error.
The \emph{Cost Function}, also called \emph{Loss Function}, as the mean square difference between the desired and the network output is minimize by using a gradient search technique. For a training data set of
pairs $[x_i,d_i]_{1}^{N_T}$, where $d_i$ is the desired output with respect the input pattern $x_i$, the cost function $J(w)$ is given as:
\begin{equation}
J(w)=\sum_{i=1}^{N_T} (d_i[n]-y_i[n])^2 = \sum_{i=1}^{N_T} e_{i}^2[n]
\end{equation}
The best training procedure is based on a wide range of examples, covering as many different cases as possible. This crucial step is performed over a fixed number of epochs due to the
fact that few examples of training involve unwanted behaviour of the network. Let us see at mathematical level how the basic Back Propagation algorithm is derived. First of all we need 
to extend the Generalized Delta Rule (GDR) for multilayer networks of multi perceptron neurons (MLP-NN) in a similar way to the Equation 3.7. Let us consider a training set composed by the following pairs: $d$ $\in$ $\mathbb{R}^{N_L \times 1}$, $x$ $\in$ $\mathbb{R}^{N_0 \times 1}$ for $n=1,2,...,N$. The weights are evaluated by minimizing a cost function as the sum of squared errors and the recursive algorithm for adaptation rule follows the same steps starting to Equation 3.6. 
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.5]{Images/neurons_k.png}
    \caption{$k^{th}$ neuron of the $l^{th}$ hidden layer.} 
\end{figure}
\\
Figure 3.5 shows the representation of the $k^{th}$ neuron of the $l^{th}$ hidden layer. Considering the inputs in this draw, the derivative of the
activation function is shown in Equation 3.9; considering the weigths, the derivative of the linear-combination neuron output is shown in Equation \ref{eq:linear_weights};
considering the previous-layer inputs, the derivative of the linear combination neuron output is shown in Equation 3.10.

\begin{equation}
\varphi'(s_{k}^{(l)}) = \frac{ \partial x_{k}^{(l)} }{ \partial s_{k}^{(l)} } = \frac{ \partial \varphi ( s_{k}^{(l)}) }{ \partial s_{k}^{(l)} }
\end{equation}
\begin{equation}
\frac{\partial(s_{k}^{(l)})}{\partial(w_{kj}^{(l)})} = \frac{\partial \bigg( \sum_{j=0}^{N_{l-1}} w_{kj}^{(l)}x_j^{l-1} \bigg)   }{ \partial w_{kj}^{(l)} } = x_{j}^{(l-1)}
\end{equation}
\begin{equation}
\frac{\partial(s_{i}^{(l+1)})}{\partial(x_{k}^{(l)})} = \frac{\partial \bigg( \sum_{j=0}^{N_{l}} w_{ij}^{(l+1)}x_j^{l} \bigg)}{ \partial x_{k}^{(l)} } = w_{ik}^{(l+1)}
\end{equation}

Using the derivative chain rule for both Equations 3.9, 3.10 we obtain:

\begin{equation}
\frac {\partial \varphi(s_{k}^{(l)})}{\partial w_{kj}^{(l)}} = \frac{ \partial \varphi (s_{k}^{(l)}) }{ \partial s_{k}^{(l)} } \cdot \frac{ \partial s_{k}^{(l)} }
{ \partial w_{kj}^{(l)} } = \varphi'(s_{k}^{(l)}) \cdot x_{j}^{(l-1)}
\end{equation}

Using the derivative chain rule according to the output layer condition $y_k$ $\equiv$ $x_k^{L}$ $=$ $\varphi(s_k^{(L)})$ we have:
\begin{equation}
\begin{aligned}
\frac {\partial \hat{J}(w)}{\partial w_{kj}^{(L)}} &= \frac{\partial }{\partial  w_{kj}^{(L)}} \Bigg [ \frac{1}{2} \sum_{k=1}^{N_{L}} (d_k - \varphi(s_k^{(L)}))^2 \Bigg ] \\
&= (d_k - \varphi(s_k^{(L)})) \cdot \frac{\partial \varphi(s_k^{(L)})}{\partial w_{kj}^{(L)}} \\
&= e_k \cdot \frac{\partial \varphi (s_k^{(L)})}{\partial s_k^{(L)}} \cdot \frac{\partial s_k^{(L)}}{\partial w_{kj}^{(L)}} \\
&= e_k \cdot \varphi'(s_k^{(L)}) \cdot x_j^{(L-1)}
\end{aligned}
\label{eq:deriv_weight}
\end{equation}

We must now extend the derivative calculation for all the weights, for the hidden layers, for the $k^{th}$ neuron of the $l^th$ hidden layer as:
\newline
\begin{equation}
\begin{aligned}
\frac {\partial \hat{J}(w)}{\partial w_{kj}^{(l)}} &= \sum_{i=1}^{N_{l+1}} \frac{\partial \hat{J}(w) }{\partial s_{i}^{(l+1)}} \cdot \frac{\partial s_{i}^{l+1}}{\partial x_k^{(l)}}
\cdot \frac{\partial x_k^{(l)}}{\partial s_k^{(l)}} \cdot \frac{\partial s_k^{(l)}}{\partial w_{kj}^{(l)}} \\
&= \sum_{i=1}^{N_{l+1}} \frac{\partial \hat{J}(w) }{\partial s_{i}^{(l+1)}} \cdot w_{ik}^{(l+1)} \cdot \varphi'(s_k^{(l)})x_j^{(l-1)}
\end{aligned}
\label{eq:extends_derive_calc}
\end{equation}
\newline
This Equation 3.14 can be expressed in a form similar to Equation 3.13 as:
\newline
\begin{equation}
\frac {\partial \hat{J}(w)}{\partial w_{kj}^{(l)}} = e_k^{(l)} \cdot \varphi'(s_k^{(l)}) \cdot x_j^{(l-1)}
\end{equation}
\newline
in which a local gradient is defined as:

\begin{equation}
\delta_i^{(l+1)} = \frac {\partial \hat{J}(w)}{\partial s_{i}^{(l+1)}}
\end{equation}
\newline
and a local error as:
\newline
\begin{equation}
e_k^{(l)} = \sum_{i=1}^{N_{l+1}} \delta_i^{(l+1)} \cdot w_{ik}^{(l+1)}
\label{eq:local_err}
\end{equation}
\\
\\
\\
The development obtained until now defines the \emph{standard back-propagation algorithm} as following:
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.5]{Images/backprop_algo.png}
    %\caption{A complete representation of Back-propagation algorithm recursion for each neuron and layer of the network.} 
\end{figure}
\newpage






\section{State Of The Art: HAR approaches}
Nowadays, many valid deep learning methodologies are widespread reaching the State Of The Art for Human Activity Recognition: in last years traditional pattern recognition approaches have made several progress. However, these methods rely heavily on manual feature extraction, which may hinder the generalization model performance. With the increasing popularity and success of deep learning methods, using these techniques to recognize human actions in \emph{"mobile and wearable"} computing scenarios has attracted large attention. In particular different approaches have been pursued with CNN and LSTM networks, or even using hybrid methodologies by combining convolutional layers with long-term memory: a model of this kind could in fact automatically extract the characteristics of the activity and classify them with some parameters of the model.

\subsection{Convolutional Neural Networks}
 When we talk about classical approaches, it is natural to immediately think about the contribution coming from Convolutional Neural Networks (Figure 3.6): they are multi-level neural networks, similar to the MultiLayer Perceptron, but with a structure made of very particular convolutive layers. \\ These type of networks can take an input image, assign importance (learnable weights and biases) to various aspects/objects in the image being able to differentiate them from each other. The preprocessing required in a CNN is much less than in other classification algorithms. While in primitive methods the filters are built by hand, with enough training, CNNs have the ability to learn these filters/features.
The architecture of a CNN is analogous to that of the connectivity model of neurons in the human brain and was inspired by the organization of the visual cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. A set of such fields overlap to cover the entire visual area. A CNN can successfully capture spatial and temporal dependencies in an image through the application of relevant filters. The architecture performs better adaptation to the image dataset by reducing the number of parameters involved and re-usability of weights. In other words, the network can be trained to better understand the sophistication of the image.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.55]{Images/cnn_general.png}
\caption{A general example of CNN structure to classify handwritten digits}
\end{figure} \\ \\
The role of CNNs is to reduce the images into a form that is easier to process, without losing the features that are critical to getting a good prediction. This is important when we need to design an architecture that is not only good at feature learning, but also scalable over massive datasets. So the goal of the convolution operation inside is to extract high-level features, such as edges, from the input image. CNNs should not be limited to a single convolution layer. Normally, the first convolutional layer is responsible for capturing low-level features such as edges, color, gradient orientation, etc. As layers are added, the architecture adapts to the high-level features as well, giving us a network that has full understanding of the images in the dataset, similar to how we would. So adding more FC layers the network is more heavy and slow, but it learns very well the relationships between classes/activities and how to generalize them better.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.55]{Images/activation_map.png}
\caption{A convolutive filter}
\end{figure}
\\ Supposing have an input image of size 28x28x1, a filter of size 5x5x1 (the depth size of the filter must always extends the depth of the input) performs a convolution by sliding over the image and computing dot products (Figure 3.7). The result between this filter and a small chunk of the image in this case will be exactly:
\begin{equation}
w^T x + b
\end{equation} in this example, 75-dimensional dot-product + bias. After sliding the filter over all the image's locations an output array of 24 x 24 x 1 is created. This vector's dimension came from all the possible locations that a 5 x 5 filter can fit on a 28 x 28 input image and corresponding to an activation map, also called feature map. The purpose of this layer is to introduce non-linearity to a system that 
basically has just been computing linear operations during the convolutional layers.\\
The following formula is used to determine the size of an activation map:
\begin{equation}
(N + 2P - F)/S+1
\end{equation}
where N is the dimension of the input image; F the size of the filter; P the padding and S the stride. Padding's parameters manage the filter's size constraint in order to allows different kernel size on the image without occur in typically dimensionality errors. Instead, the Stride's parameters controls how the filter convolves around the input volume, in particular it reduces the size of the next network's layer and allows to decide how much overlap you want between two output values in a layer. CNNs have different types of activation functions just as the Figure 3.8 shows. In the past, non-linear functions like \emph{Sigmoid} and \emph{Tanh} were used. The \emph{Sigmoid} function is a differentiable function defined in the range between 0 and 1. It is possible to find the slope of the sigmoid curve at any two points and its curve looks like a S-shape (Figure 3.8); the \emph{Tanh} function, also called \emph{hyperbolic tangent} function, is also like logistic sigmoid but in this case it lies on the range from -1 to 1. Also in this case, this function is differentiable and is often used in classification tasks between two classes. However, with the progression of the technology it has been discovered that networks with \emph{ReLU} activation function, i.e., rectified linear unit function, work far better due to the ability to train a lot faster without making a significant difference to the accuracy. This activation layer increases the non-linearity inside the model by applying the function $f(x) = max(0, x)$, with a range from 0 to infinity, to all of the input values. In this way all the negative activations will be assigned to 0.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.55]{Images/activation_funct.png}
\caption{Activation functions}
\end{figure}
After some activation layers, a pooling layer is typically applied. In case of \emph{max pooling}, this layer takes a filter and a stride of the same length and applies it to the input volume and outputs the maximum number in every convoluted filter regions. Sometimes is convenient to apply an average pooling instead of max pooling (this layer is used on recent architectures in place of \emph{Fully Connected} layers), in this case the output depends on each input (in Figure 3.9).
\begin{figure}[ht]
\centering
\includegraphics[scale=0.55]{Images/pooling.png}
\caption{Pooling layer}
\end{figure}
As mentioned before, Fully Connected layers contain neurons that connect to the entire input volume (as in ordinary neural networks) in which the activation function
can be computed as a matrix multiplication followed by a bias offset.
\\ \\ \\
In the field of HAR all these qualities can be very helpful. Actually there are several ways to approach convolutional networks in this area: for example you might think to implement a 1D CNN for time series classification tasks, or, as in our study case use the methodology of \emph{"RGB-D based Activity Recognition"}. Indeed this treats the data coming from the dataset as four-channels images to be classified just giving flatten operations: as it is discussed in the next section, in our case we will extract a sequence of 19 joints (3D) from the human skeleton, in cascade for each frame (sampled to 50 from 250) of the current activity. The network will catalog this sequence, after flatten operations (the sequence becomes 50x57), as a barcode image (Figure 3.10), easy to manage. This allows us to forget the time series factor (usually necessary in HAR) and all the problems mentioned above related to it.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.38]{Images/rgbd.png}
\caption{How an extracted set of skeleton joints could be seen as a barcode by CNN}
\end{figure}\\\\
So it is amazing how this approach makes possible to set up the problem of cataloging different activities as in reality a classification of different images (bar-code) each one representing the performance of a certain action. The convolutional levels will thus be able to generalize the concept of activity very well, by carrying out a classical image classification task. This is the reason why deep learning enchants, we can turn the tools we use totally scalable regardless of what we are doing.

\subsection{Long Short-Time Memory}
Human beings do not start their thinking from scratch every second. As you read this essay, understand each word based on your understanding of the previous words. It is well established that our thoughts have persistence. Traditional neural networks can't do this, and it seems like a big gap. For example, imagine you want to classify what kind of event is happening for each frame of a video that captures some activity. In terms of timing, it's not clear how a traditional neural network could use its reasoning about earlier events in the movie to inform later ones. Recurrent Neural Networks (RNNs) address this problem (Figure 3.11). They are networks with cycles within them, allowing information to persist. This chain-like nature reveals that they are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data.\\ In recent years, there have been incredible successes in applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning, and so on. In particular, however, these successes are due to the use of LSTMs, a very special type of recurrent neural network that works, for many tasks, much much better than the standard version. Almost all of the exciting results based on recurrent neural networks are obtained with them.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{Images/rnn.png}
\caption{Generic RNNs structure.\\ A piece of neural network, "A", looks at some input x\_t and produces a value h\_t. A loop allows information to pass from one step of the network to the next.}
\end{figure}\\
Long Short-Time Memory networks are introduced to solve the \emph{"long-term dependencies"} learning problem which RNNs present (where big scenarios require more context): remembering information for long periods of time is practically their default behavior, not something they struggle to learn. In this way the Vanishing Gradients problem is solved too (when the gradients of loss functions become too small, approaches zero, then the network becomes increasingly hard to train, as the weights and biases of the initial layers are not updated effectively with the training sessions.\\
All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer (Figure 3.12 A). LSTMs also have this chain-like structure, but the repeating module has a different structure (Figure 3.12 B). Instead of having a single neural network layer, there are four, interacting in a very special way.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.29]{Images/rnn_tanh.png} \includegraphics[scale=0.2]{Images/legend_lstm.png}\\
(A)\\
\includegraphics[scale=0.29]{Images/lstm_tanh.png} \includegraphics[scale=0.2]{Images/legend_lstm.png}\\
(B)\\
\caption{Repeating module mechanism for (A) RNNs and for (B) LSTMs}
\end{figure}
\\\\
The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged. The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.
Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation. The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means \emph{“let nothing through”}, while a value of one means \emph{“let everything through”}.
\\ \\
All these properties and feedback connections become crucial for Time Series Clssification tasks like HAR: LSTM units each consist of a cell and three regulators/gates, namely an input and output gate, and a forget gate. Each cell remembers values of arbitrary timeintervals, while the gates regulate the information flow both from and to the cells. So passing in input the sequence of human joints extracted from the skeleton (50x19x3) of each sample of the dataset used in our case, this type of network is able to understand and classify a sequence of movements, cataloguing it as one of the present activities.

\subsection{Comparisons}
Now that we have seen in detail the characteristics and operation of these two powerful types of networks and above all we have outlined the potential of both for activity recognition tasks, the question is: which one is better to use? Is there one that performs better? Obviously the most immediate answer is \emph{"it always depends"}. It depends on the specific problem, the data available, and the time you're willing to spend. As we have seen in the previous sections, CNNs are able to do very well the task of predicting data sequences, which is what LSTMs were designed for; especially given the latest results found in the State Of The Art CNNs, sometimes, can also do this in a faster and more computationally efficient way. It is that \emp{sometimes} the reason why the answer to the above question is \emp{"it depends"}.\\
For instance CNNs have many well known disadvantages points in Image Classification: 
\begin{itemize}
    \item Classification of Images with different positions or POV.\\ When objects are hidden to some degree by other objects or colored, the human visual system finds signs and other information to identify what we are seeing. Creating a CNN that has the ability to recognize objects at the same level as humans has proven difficult. Regardless of where the object is in the image, a well-trained CNN can identify the object in the image. But if the object in the image consists of rotations and scaling, then the CNN will have difficulty identifying the object in the image. These type of networks have great performance while classifying images which are very similar to the dataset . However, If the images contain some degree of tilt or rotation then CNNs usually have difficulty in classifying the image. This can be solved by adding different variations to the image during the training process otherwise known as Data Augmentation (flipping, cropping, rotating, ..., images). However the latter doesn't solve the worst case scenario as real life situations have complex pixel manipulation like a crumpled T-shirt or an inverted chair.

    \item Adversarial examples\\ If the CNN takes an image with noise, it will recognize the image as a completely different image while the human visual system will identify it as the same image with noise. This is because they use very different information than a normal visual system to recognize images. Slightly modified images are also known as "opponent examples" (Figure 3.13).
    
    \item Coordinate Frame \\ CNNs do not have coordinate frames which are a basic component of human vision. Coordinate frame is basically a mental model which keeps track of the orientation and different features of an object. The images as visualized by CNN do not have any internal representations of components and their part-whole relationships. 
    
    \item Other minor problems: \\ 1) They become significantly slower when using operations such as maxpool;\\ 2) If the CNN has several layers then the training process takes a lot of time if the computer doesn’t consist of a good GPU; \\ 3)They requires a large Dataset to process and train the neural network.
\end{itemize}
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/noise_CNN.png}
\caption{Example of \emp{adversarial examples} in Image Classification}
\end{figure}  \\
Despite all these problems, today CNNs are still one of the most advanced and useful tools in the field of deep learning. In fact, when we talk about LSTMs there are still a lot of disadvantages here too:
\begin{itemize}
    \item Vanishing Gradients problem not completely solved. Still we have a sequential path from older past cells to the current one. This path is very complicated because it has additive and forget branches attached to it. No question LSTM and derivatives (like GRU...) are able to learn a lot of longer term information! But they can remember sequences of 100s, not 1000 or more.
    \item Note Hardware \emp{friendly}. They take a lot of resources we do not have to train these network fast. Also it takes much resources to run these model in the cloud, and given that the demand for speech-to-text is growing rapidly, the cloud is not scalable.
    \item Memory-bandwidth limited problems. The processing unit(s) need as much memory bandwidth as the number of operations/s they can provide, making it impossible to fully utilize them. The external bandwidth is never going to be enough, and a way to slightly ameliorate the problem is to use internal fast caches with high bandwidth. The best way is to use techniques that do not require large amount of parameters to be moved back and forth from memory, or that can be re-used for multiple computation per byte transferred (high arithmetic intensity).
\end{itemize} 
\\ One of the reasons why people prefer to use CNNs over LSTM is the \emp{amount of training time}. The current generation of popular hardware for deep learning are basically Nvidia graphics cards, and they are optimized to process 2D-3D data with extreme parallelism and speed, which CNNs use. LSTMs on the other hand, process things more sequentially, so the deep learning hardware doesn't increase its speed by much, especially during the network training phase. So there is a drastic difference in training time between the two architectures when using large datasets: when the information to be stored becomes enormous, it is necessary to incrementally expand the number of cells and layers in an LSTM network, so that it can guarantee good results, but with a great increase in training slowness (heavy network). As demonstrated in numerous works (SOTA) instead CNNs are more shapable, ensuring great stability even with a very large number of information within them; obviously the larger the network the more the training speed decreases, but this is a hurdle that can be overcome mainly by using graphics cards as mentioned before. Neural networks, on the other hand, would not perform at all, as they need a lot of data because they are "trying" to find the hidden relationships between inputs. If they don't have enough data, they won't find the relationships or worse, they will accidentally find bogus relationships that aren't really there.\\
In addition, as always in this area, the choice of the dataset according to the task that you want to perform is fundamental: there are types of datasets and tasks where it is better to focus on LSTM, such as Sequence-to-Sequence LSTM models which are the state of the technique for translations, while in many others, as happens also for example in activity recognition, which we will see implemented in later chapters, sometimes it is better to use CNNs. Finally, it must be said that the continuous research is always trying to innovate, even combining CNN and LSTM, using attentional mechanisms, etc., in order to create hybrid approaches that can focus on the strengths of both, while eliminating the weaknesses, so there are and there will always be new ways to explore in this complicated scenario. There is a clear trend in the literature away from LSTMs, but that doesn't mean they have disappeared and everything seen so far provides hints as to why. It seems that the CNN vs. LSTM game is not over yet.

\newpage
\section{Skeleton estimation for Activity Recognition}
The human skeleton, as a compact representation of human action, has received increasing attention in recent years. Many skeleton-based action recognition methods adopt convolutional graph networks (GCNs) to extract features over human skeletons. Despite the positive results shown in previous works, GCN-based methods are subject to limitations in robustness, interoperability, and scalability; some innovative approaches in this regard involve a 3D heat-map stack instead of a sequence of graphs as the basic representation of human skeletons, succeeding well on spatiotemporal feature learning and being robust against pose estimation noise.\\
In this work another State Of The Art approach is followed: the benchmark paper (Domain Analysis of End-to-end Recovery of Human Shape and Pose) presents an end-to-end framework to recover a complete 3D mesh of a human body from a single RGB image. As illustrated in Figure 3.14, estimating a 3D mesh opens the door to a wide range of applications such as foreground and part segmentation, which goes beyond what is practical with a simple skeleton. The resulting mesh can be immediately used by animators, modified, measured, manipulated and retargeted. This type of output is also holistic, i.e., the entire 3D body can always be inferred even in cases of occlusion and truncation (this solves the problems that arise if you have to process frames from a partial shot of the subject).
\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.21]{Images/skeleton1.png}\\
\includegraphics[scale=0.21]{Images/skeleton2.png}
\caption{Two Human Mesh Recovery output examples}
\end{figure}\\
I decided to exploit this approach in my work for several reasons: (1) today it is one of the most recent and performing tools in terms of accuracy for human skeletons recovery task; (2) it provides in addition to the mesh of the human also its \emph{"estimated"} joints both 2D and 3D; (3) it is a framework already pursued by Konica Minolta in the past, obtaining good results; (4) it releases pretrained models and open source code to use and be able to modify according to one's needs.

\subsection{End-to-end recovery of human shape and pose}
In the reference paper, it's used the generative human body model, SMPL, which parametrizes the mesh through 3D joint angles and a two-dimensional linear shape space. While most approaches focus on recovering 3D joint positions, here we argue that these alone are not the complete story: joints are scattered, while the human body is defined by a surface in 3D space. Moreover, the positions of the joints alone do not constrain the entire DoF of each joint. Therefore, relative 3D rotation matrices are produced for each joint in the kinematic tree, capturing information about the 3D orientation of the head and limbs. Predicting rotations also ensures that limbs are symmetric and of valid length. The model in question implicitly learns joint angle constraints from 3D body model datasets.\\
This approach targets mesh reconstruction that addresses the following challenges: (A) lack of large-scale 3D annotations for in-the-wild images; (B) problem of depth ambiguity, which can confuse the mesh. A key insight is that there are large-scale 2D keypoint annotations of in-the-wild images and a separate large-scale 3D mesh dataset of people with various poses and shapes. Our key contribution is to exploit these unpaired 2D keypoint annotations and 3D scans in a conditional and generative way. The idea is that, given an image, the network must infer the 3D mesh parameters and the camera so that the 3D keypoints match the 2D keypoints annotated after projection. To address ambiguities, these parameters are sent to a discriminator network, whose task is to determine whether the 3D parameters correspond to real human bodies or not. Then the network is encouraged to produce parameters on the human manifold and the discriminator acts as a weak supervisor. The network involves learning the angular limits for each joint and is discouraged from creating people with unusual body shapes.
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.42]{Images/skeleton_gan.png}
\caption{Framework of the network used for skeleton estimation}
\end{figure} \\
As it is shown in Figure 3.15, an image $I$ is passed through a convolutional encoder. This is sent to an iterative 3D regression module that infers the latent 3D human representation that minimizes the joint reprojection error joint reprojection error. The 3D parameters are also sent to the $D$ discriminator, human shape and pose. The $Regression$ step is added to the network to overcome the challenge related to the prediction of rotation matrices: while most approaches formulate rotation estimation as a classification problem by dividing angles into bins (sacrificing precision due to discretization, here instead it is proposed to directly regress these values iteratively through a feedback chain.\\
Due to the rich representation of the 3D mesh model, this data-driven prior can capture joint angle limits, anthropometric constraints (e.g. height, weight, bone ratios), and subsumes the geometric priors used by models that only predict 3D joint locations. When ground truth 3D information is available, we may use it as an intermediate loss. The overall objective function will be:
\begin{equation}
L = \lambda (L_{reproj} + g L_{3D}) + L_{adv}
\end{equation} 
where $\lambda$ controls the relative importance of each objective, g is an indicator function that is 1 if ground truth 3D is available for an image and 0 otherwise. \\ \\ More technical details of this approach are described in the reference paper. However, this goes beyond our scope now, so we will focus on the practical side, takling about the pretrained model this network gives us and how skeleton joints can be obtained through predictions on our activities dataset.


\subsection{Skeleton joints extraction}
As we mentioned earlier, the reference paper provides free code for research purposes at the url: \emph{https://github.com/akanazawa/hmr}.\\ All the basic code is proposed here both to retrain the network from scratch and to use several pretrained models to perform inference on custom images. Following github instruction is very easy to understand the network and redesign it such that we can extract what we need as well. Unfortunately this repository is 3 years old, and as we know, in the world of deep learning this is a lot of time. In fact the original code was made using: tensorflow1.3 and python2.7, both outdated today. So the first thing to do was looking for an upgrade at: \emph{https://github.com/russoale/hmr2.0} it was possible to find a new, updated version (up to 2020) of the one described before, where it was sufficient to make just a few internal changes to make it work, as it is still in progress. In particular this repository is implemented with tensorflow2.1 and python3.6 . Therefore just following their github guidelines it is very easy to 1) setup the environment and requirements 2) download the pretrained model 3) running the engine and test the skeleton estimator for the first time: relative results are shown in the figure below. \begin{figure}[ht]
\centering    
\includegraphics[scale=0.185]{Images/skeleton-engine.jpeg}
\caption{Testing the Skeleton Estimation engine on synthetic data frame}
\end{figure}
\\
Once set up the engine, a brief study on the code and on the inner dynamics, has allowed me to verify that this network returns in output, among the many possible, the 3D joints of the human skeleton, fundamental for the activity recognition. The methodology used for this data preprocessing phase is shown in detail in the next chapter. \\\\ This engine allows the retrieval of 19 joints related to the human body (Figure 3.17):
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.55]{Images/human-joints.png}
\caption{The 19 human 3D joints (red points)}
\end{figure}
\\
These are given as \emph{numpy array} in a list like this: $\#$frames $\times$ $\#$joints $\times$ 3-dimensions ($50\times19\times3$). This sequence of numbers will play a crucial role in performing the activity recognition task, and will become our dataset once extracted for each frame of each sample.\\
This mechanism has several strong points: the first is that, even in case of partial cover of a part of the body (which can happen especially when performing activities), therefore not seen by the camera, all 19 joints are predicted the same in output (thanks to the theoretical principles discussed above). Obviously, however, at least 60\% of the body must always be visible in order to apply the skeleton estimation.\\ Another merit is that it is able to recognize the human shape also on synthetic dataset and to predict joints that will be always the same (for any domain). Since our dataset will be composed precisely of these joints, this becomes crucial because it reduces many noise problems that often afflict the transition from the synthetic to the real domain, especially when it comes to image classification (Figure 3.18). However, if the movements of the activities are not randomized properly, the neural network that is used can still overfit during training causing serious problems when switching to a synthetic dataset where the sequences of joints are always different.\\
Finally, it is important to note that this type of inference is shape-invariant: whether we have at the center of the scene characters of different sizes (fatter, taller, children ...) can still extract the joints effectively, performing a sort of internal shape-normalization. Moreover, for the case of 3D joints it manages not to lose the depth measure despite this transformation. This engine seems to be really perfect!\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.6]{Images/joints-compare.png}
\caption{Outputs given from synthetic and real domain.\\ Although these two samples belong to different actions, pov and domains (real on the left, synthetic on the right), we will always have a skeleton joints structure as output}
\end{figure}
\\
A very small problem, however, has been identified. Despite the reference paper is rich in information, nowhere is the nature of the output data specified: do these numbers have a range through which they were normalized? It's a fundamental thing to know for those who want to insert them in a ML network (CNN, LSTM...). Obviously this obstacle is easily solvable by scouting all the data obtained in the dataset and calculating the maxima and minima of each sequence in order to have an overall value. After this test the data seem to belong to the range [-1.195, 0.887]: it is a really strange interval! Obviously, as it is shown in the following section, depending on the network we used, these data numbers will be normalized before being processed.

\begin{figure}[ht]
\centering    
\includegraphics[scale=0.9]{Images/x_train.png}
\caption{Partial data values (training set) after joints extraction. \\ Remember that we deal with a set of [Sx50x19x3] where S is the number of samples in current set (training\_set = 1200, validation\_set = 300)}
\end{figure}


\section{Introduction to Continual Learning}
The last decade has marked a profound change in the way we perceive and talk about Artificial Intelligence. Just think of the recent deep learning techniques that have literally swept away previous AI approaches. However, most of the research related to "Classical" Deep Learning today is still conducted solving specific and isolated tasks that would hardly lead to a more long-term vision equipped with common sense and versatility. In fact, despite the fact that these approaches allow to achieve incredible results in so many fields of research, Deep Learning remains static, dataset dependent, non-adaptable and non-scalable.\\
For instance, if we wanted to train a CNN to perform a certain classification task T having at our disposal a dataset with 50 different classes, as we know, it is possible to do it (even well) and also in a short time if we have a GPU. But if, after to have trained the model to recognize these 10 classes we wanted to widen the dataset also only adding a new class, what should we do? If we train the pretrained model on the new class, the weights of this one would totally move on the new one, forgetting the first 10 learned classes: the only way is to re-train the network with 11 classes from scratch. But what happens, if the classes to add are multiple and/or you don't have a graphic card, therefore increasing exponentially the training times?\\ Continual Learning (CL) is thus introduced, built on the idea of continuously and adaptively learning about the external world and enabling the incremental autonomous development of increasingly complex skills and knowledge. This is in fact able to smoothly update the prediction model to account for different tasks and data distributions, but still be able to reuse and retain useful knowledge and skills over time.
Thus, CL is the only paradigm that forces us to deal with a higher, more realistic time scale where data (and tasks) only become available over time, we do not have access to previous perception data, and it is imperative to build on top of previously learned knowledge. \\ \\ 
\subsection{Avoiding Catastrophic Forgetting with the\\ Elastic Weight Consolidation}
In the field of AI, agents must be able to learn and remember many different tasks, and this becomes particularly difficult in real-world dynamics: the sequence of tasks may not be explicitly labeled, tasks may change unpredictably, and any single task may not repeat for long intervals of time. Thus, intelligent agents must demonstrate a capacity for continuous learning: that is, as we mentioned earlier, the ability to learn consecutive tasks without forgetting how to perform previously trained tasks (Figure 3.20). Continual learning poses particular challenges for artificial neural networks because of the tendency to abruptly lose knowledge of previously learned tasks (e.g., task A) when information relevant to the current task (e.g., task B) is incorporated. This phenomenon, called Catastrophic Forgetting, occurs in particular when the network is trained sequentially on multiple tasks, because the weights in the network that are important for task A are changed to meet the goals of task B. Unfortunately, present approaches today to remedy this have typically ensured that data from all tasks are simultaneously available during training. By interleaving data from multiple tasks during learning, forgetting does not occur because network weights can be jointly optimized for performance on all tasks. This approach is often presented as a multitask learning paradigm. Unfortunately, if tasks are presented sequentially, multitask learning can only be used if data is recorded by an episodic memory system and replayed in the network during training. Thus, this is ineffective for learning a large number of tasks, as it would require an amount of stored and replayed memories proportional to the number of tasks. Thus, the scarcity of algorithms that support Continual Learning remains a key problem for the development of artificial general intelligence.
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.5]{Images/CL_task_space_general.png}
\caption{Task space for different DL techniques}
\end{figure}
In stark contrast to artificial neural networks, humans and other animals appear to be able to learn continuously. Recent evidence suggests that the mammalian brain can avoid Catastrophic Forgetting by protecting previously acquired knowledge in neocortical circuits. When an animal acquires something new (such as completing a certain task), a portion of the excitatory synapses are strengthened; this manifests as an increase in the volume of individual dendritic spines of neurons. This enlargement persists despite the subsequent learning of other tasks, representing the maintenance of performance several months later. This provides causal evidence that the neural mechanisms that support the protection of these strengthened synapses are critical for the maintenance of task performance. In the referenced paper (for my thesis) "Overcoming catastrophic forgetting in neural networks", an algorithm analogous to synaptic consolidation is developed for ANNs, called Elastic Weight Consolidation or simply EWC.\\ \\ Denote parameters of layers of a deep neural network (DNN) with $\theta$. Training DNNs generates a mapping between the input distribution space and target distribution space. This is done by finding out an optimum $\theta = \theta$* which results in the least error in the training objective. The term many configurations can be interpreted as a solution space around the most optimum θ with acceptable error in the learned mapping. Note that in figures to follow, the shaded ellipses represent the solution of individual tasks where as the overlapping region of multiple ellipses represents the common solution space for all tasks.\\ Let’s begin with a simple case of two tasks, task A and task B. To have a configuration of parameters that performs well for both A and B, the network should be able to pick $\theta$ from the overlapping region of the individual solution spaces (Figure 3.21 a). This is with the assumption that there is always an overlapping region for the solution spaces of all tasks for the network to learn them sequentially. A case of four tasks has been illustrated in Figure 3.21 b. In the first instance, the network can learn any $\theta = \theta_A$: A which well performs for A itself. If a task B is added, the network should pick up a $\theta = \theta_{A,B}$. To learn a set of parameters that lies in this overlapping region, EWC presents a method of selective regularization of $\theta$ .
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.52]{Images/CL_task_space_overlap.png}
\caption{Overlap of possible configurations of $\theta^*$.\\ The overlapping space represents an optimum parameter region where the network performs without any catastrophic degradation on previous tasks}
\end{figure}
After learning A, this regularization method identifies which parameters are important to A, and then penalizes any changes made to the network parameters based on their importance while learning B. \\ \\ To formulate the objective, we begin with a Bayesian approach needed to estimate the network parameters $\theta$. More specifically, given data $\Sigma$, we want to learn the posterior probability distribution function $p(\theta|\Sigma)$. So we could write:
\begin{equation}
p(\theta|\Sigma) = \frac{p(\Sigma|\theta) p(\theta)}{p(\Sigma)}
\end{equation} 
since maximizing a function is same as maximizing its logarithm, we take $log()$ of (3.21) as following:
\begin{equation}
log(p(\theta|\Sigma)) = log(p(\Sigma|\theta)) + log(p(\theta) - log(p(\Sigma)
\end{equation} 
to train the neural network on $\Sigma$, the objective function to be optimized over the log-likelihood function
\begin{equation}
argmax_\theta \{l(\theta) = log(p(\theta|\Sigma))\}
\end{equation} 
For the case of given two independent tasks such that (with B appearing in sequence after A), (3.22) can be written as
\begin{equation} 
\begin{split}
log(p(\theta|\Sigma)) = log(p(B|A,\Sigma)) + log(p(\theta|A)) - log(p(B|A)) \\= log(p(B|\theta)) + log(p(\theta|A)) - log(p(B))
\end{split}
\end{equation} 
Following (3.21), $p(B|\theta)$ is the loss for current task B, $p(B)$ is the likelihood for B, and now posterior $p(\theta|A)$ for A becomes prior for B.

\subsection{Intractability of posterior of A and it’s approximation}
Referring (3.24), it can be observed that we have to deal with the function $p(\theta|A)$. This is the posterior function for A which contains the information about the parameters that explain A using the given network. As discussed in several State Of The Art papers, this posterior function is known to be \emph{intractable}. Basically, this intractability can be interpreted as the function not existing in some interpretable form. Hence, it is difficult to estimate its quantiles. Next as the posterior is difficult to analyze in its present form, in the EWC reference paper it is approximated using Laplace approximation. In simple words, Laplace methodology is employed to find a normal distribution approximation to a continuous probability density distribution (see Fig. 3.22). Assuming $p(\theta|A)$ is smooth and majorly peaked around its point of maxima (i.e. $\theta^*_A$), it can be approximated with a normal distribution with \emph{mean} $\theta_A$* and \emph{variance} $[\mathbb{I}_A^{-1}]$.
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.7]{Images/posterior_intractability.png}
\caption{Laplacian Approximation of true posterior pdf. \\ $\mathbb{I}_A$is the Fisher Information matrix, explained following}
\end{figure}
\subsection{Importance of parameters using Fisher Information matrix}
Given $\theta^*_A$ , the term $log(p(\theta|A))$ represents the log-likelihood of posterior pdf $p(\theta|A)$: this term represents the inverse of the Fisher Information Matrix (FIM) $\mathbb{I}_A$. So it is possible, thanks to the Laplacian Approximation, get: $p(\theta|A) \sim N (\theta^*_A, \mathbb{I}_A^{-1})$. Further, as FIM can also be computed from first order derivatives, we can avoid the Hessian operations using the following property:
\begin{equation} 
\begin{split}
\mathbb{I}_A = \mathbb{E} [-\frac{\partial^{2} log(p(\theta|A)) }{\partial^{2} \theta} |_{\theta^*_A}] \\ = \mathbb{E} [((\frac{\partial log(p(\theta|A)) }{\partial \theta})(\frac{\partial log(p(\theta|A)) }{\partial \theta})^T)|_{\theta^*_A}]
\end{split}
\end{equation} 
now given Laplacian approximations it is possible to write (3.24) into:
\begin{equation} 
\begin{split}
log(p(\theta|\Sigma)) = log(p(B|\theta)) + log(p(\theta|A)) - log(p(B)) \\ = log(p(B|\theta)) + \frac{\lambda}{2}(\theta - \theta^*_A)^T (\frac{\partial^{2} log(p(\theta|A)) }{\partial^{2} \theta} |_{\theta^*_A})(\theta - \theta^*_A) + \epsilon^'
\end{split}
\end{equation} 

where $\epsilon^'$ accounts for all constants and $\lambda$ is a hyper-parameter introduced to have a trade off between learning B and not forgetting A. Simplifying more:

\begin{equation} 
\begin{split}
log(p(\theta|\Sigma)) = log(p(B|\theta)) + \frac{\lambda}{2}(\theta - \theta^*_A)^T (\frac{\partial^{2} log(p(\theta|A)) }{\partial^{2} \theta} |_{\theta^*_A})(\theta - \theta^*_A) + \epsilon^' \\ = log(p(\theta|\Sigma)) = log(p(B|\theta)) + log(p(\theta|A)) - log(p(B)) \\ = log(p(B|\theta)) - \frac{\lambda}{2}(\theta - \theta^*_A)^T \mathbb{I}_A (\theta - \theta^*_A) + \epsilon^' \\ 
\rightarrow l(\theta) = l_B(\theta) - \frac{\lambda}{2}(\theta - \theta^*_A)^T \mathbb{I}_A (\theta - \theta^*_A) + \epsilon^'
\end{split}
\end{equation} 

where $l(\theta)$ is the overall loss equal to $l_B(\theta)$ which is the loss for the new task incremented by $(\theta - \theta^*_A)^T \mathbb{I}_A (\theta - \theta^*_A)$ that is the regularization loss (weight regularizer): this will be very important for avoid the forgetting because the regularization loss will act as an anchor for the weights of the previously learned model. \\Here we can understand the importance of the Fisher Matrix in Continual Learning: we know a network has learned a task when its objective has min loss and the curvature of such surfaces represent the sensitivity of the network with respect to the optimum $\theta^*$. This sensitivity can be determined by looking at the direction along which $\theta^*$ changes. This implies the curvature is inversely proportional to change in $\theta^*$. So the larger the curvature, the larger a small increment can turn out to be in terms of increased loss. The curvature of a curve is denoted by its Hessiana and so in our case, since the second derivative is of the log likelihood function of the posterior pdf, the FIM comes into the picture. This can tell us which parameter is important for the previous task, since its corresponding element will have a large value, indicating greater importance. In the next chapter we will see how this is implemented and structured.

\subsection{Further considerations}
While not already at its explosion, Continual Learning has been getting more and more attention in the Deep Learning community and in the last two years very good contributions have come out. In particular, today, the EWC is one of the best performing methodologies in this area, being able to alleviate Catastrophic Forgetting by regularizing the parameters of a network trained on previous tasks by penalizing any changes based on their importance. This importance is indicated by Fisher's information matrix, i.e., after a network is trained on one task, fine-tuning on the next task is performed according to (3.27). \\ Note that the regularization task sometimes is a very complicated process, not so immediate to carry out as we will see in the next chapters. Indeed three different possibilities are shown in Figure 3.23. In Figure 3.23(a), we have the case when we simply fine-tune the network on the subsequent tasks (classical DL approach). This makes the network learn the optimum parameters according to the current task, resulting in forgetting the previous to learn the new one. Figure 3.23(b) represents the case when we apply a stringent regularization to the parameters learnt from the previous tasks. This method does not compute the importance of the parameters and penalizes all of them equally. This could lead the network not only forget the previous task, but also to not learn the new one. Finally, Fig. 3.23(c) refers to the EWC methodology of computing the importance of parameters (FIM) before fine-tuning on new tasks. This ensures the network to learn the optimum parameters performing well for all tasks and hence, lies in the overlapping region of the solution spaces of the tasks in the given sequence (CL best case scenario). \\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.5]{Images/ewc-task.png}
\caption{Different scenarios of sequential training for 2 tasks}
\end{figure} 
\\ \\
Finally, now we have seen also at a theoretical level what are the internal mechanisms of this innovative approach, we can sum up pros and cons of Continual Learning. \\ \\ Pros:
\begin{itemize}
    \item Contrasting Catastrophic Forgetting is possible in many ways and not only through careful hyper-parametrizations or regularization techniques;
    \item CL can be used for complex problems as Computer Vision, Supervised or with Reinforce learning, overcoming the many problems coming classical DL approaches related to static and non-scalable approaches;
    \item Considering that, these are fairly new techniques, accuracy results are impressively high, almost in line with other (classical) techniques which have access to previously encountered data.
\end{itemize}\\ 
Cons:
\begin{itemize}
    \item It’s not yet completely clear how evaluate CL techniques and a more formalized framework may be needed;
    \item There are few papers present at the state of the art, and poor documentation about implementation (code level). Moreover they are based on relatively simple and well known dataset in the field of deep learning; for instance dataset lik Cifar100 and ImageNet are often used for image classification tasks (Figure 3.24). Furthermore papers about Human Activity Recognition are rare;
    \item It’s not clear how to behave after the saturation of the capability of the model (no more neurons available), neither how to selectively forget.
\end{itemize}
Nevertheless, this approach seems to be destined to become more and more central in the world of DL, because its benefits are really unique and useful to make a huge leap in terms of technological development in this area.\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.4]{Images/benchmark_cl.png}
\caption{Continual Learning Benchmarks (paperswithcode.com)}
\end{figure} 

\chapter{The proposed method}
{\fontsize{16}{18}\selectfont 
  \textbf{Self supervised Continual Learning from \\3D human skeleton joints}
}
\\
\lettrine[lines=2, findent=3pt, nindent=0pt]{W}{}hen you are in complex scenarios such as AI, in order to fully understand a concept, you must first obtain a top view of the problem, marking the crucial points necessary for the realization of that concept: keeping your mind clean and tidy is the key to understand and finalize everything.
So this chapter shows the implemented structure and all the detailed steps that allowed me to finalize the work in terms of code development. In fact, following tips and objectives proposed by Konica, step by step my project took shape. Following  the proposed methodology is described: \emph{"A self-supervised Continual Learning approach for HAR using 3D human skeleton joints"}.\\\\ 
As we have seen in chapter 2, thanks to the Unity simulator it has been possible to generate different activities (Hands Up, Waving Hands and Falling down) in a well randomized environment to guarantee a varied and not repetitive dataset. This (called \emph{Unity\_dataset}) is composed by 2000 samples per activity (500 x 4 cameras) characterized by RGB frames (250) plus json  files containing the bounding boxes (labels captures) of the characters inside. It must be said that for each scene, we have only one labeled character, this because our engine is based exclusively on "Single-Person" Activity Recognition. With more than one person, the bounding boxes would be more than one per scene and consequently the skeleton estimator could be confused by this: a "Multi-Person" HAR remains today an open task and implementable in the future.\\ Now let's go back to our Unity\_dataset: the methodology proposed and applied throughout this work in fact starts right here. Since this type of raw dataset cannot be used directly for HAR, some preprocessing operations are required: the first part will transform Unity\_dataset into a full-fledged dataset (which we call \emph{OID}), but still temporary, characterized by RGB folder and Label folder (with bounding boxes expressed as txt files), then ordered and characterized by a training\_set (80\% of data) and validation\_set (20\% of data); the second part of preprocessing deals with the Skeleton Estimation and joint extraction, creating a new dataset (the last one, called \emph{ar\_dataset}); more details of these phases are well described in the next section. This paragraph concerns the whole preprocessing part which allowed the data parsing and the recovery of the extracted joints. Note that this process was very time consuming, both in code creation and in running the main scripts (huge computational calculation required). \\ \\
Section 4.2 instead is about networks (CNN and LSTM) implementation from scratch for Activity Recognition. To accomplish this task I implemented an LSTM and a CNN with respectively very simple, but well designed structures. As we saw in the previous chapter, while the LSTM follows the temporal logic by storing sequences of joints that are passed to it for each sample of the dataset, the CNN is designed to classify activities through an RGB-D identification of bar-code images created by flattening the input.\\
We can resume the main steps of this procedure (Figure 4.1) as following:
\begin{itemize}
    \item Dataset Generation with Unity 3D simulator
    \item Dataset parsing and Skeleton Estimation
    \item Neural Networks building for Activity Recognition
\end{itemize}

\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/high_architecture.png}
\caption{Activity Recognition: high architecture. \\ We can see 1) data coming from Unity, going through 2) data preprocessing steps (OID and ar\_dataset are generated); 3) then data is normalized and put as input for AR networks; 4) whether CL is on or not, we can obtain as output the trained model}
\end{figure} Furthermore in the Section 4.1.3 a different preprocessing approach is shown: up to here we just treated synthetic dataset handling, so a universal dataset preprocessing (for both real and synthetic domain) is presented, filtering real frames with external CV tools like YOLO for person detection. \\\\ In Section 4.3 instead a custom EWC implementation is presented, showing step by step each developed key point and the related complications have arisen, up to the building of a fully automatic pipeline to perform this Continual Learning approach. It is important to remember that HAR was developed through two parallel approaches: classical one and with Continual Learning (EWC); so we need to add the following point to the previous 3 steps:
\begin{itemize}
    \item Classical vs Continual Learning approach
\end{itemize} Therefore this implementation allows to switch these two possible ways at will; this is obviously important to get a direct comparison and see how time behaves with respect to each other. Everything has been implemented in a totally parametric way: in fact each project sub folder (lstm\_estimator, cnn\_estimator, dataset\_tools...) has a configuration file (config.ini) where it is possible to specify paths, variables, flags and so on. Section 4.4 explains all the guidelines to build up the EWC approach in parallel to the classic. \\\\ In conclusion, a further section (4.4) has been added in this chapter, where I choose to talk about the two main tools I used at software and frameworks level (Tensorflow and Docker) highlighting their importance in this work. Some installation steps are mentioned too.


\section{Automated synthetic dataset generation}

As we mentioned before, this section is essential if we want to perform the HAR task. Why Data Preprcessing is so important?\\
It is a process that prepares the raw data and makes it suitable for a Machine Learning model. It is the first and crucial step during the creation of a machine learning model. When creating a machine learning project, it's not always a case of coming across clean, formatted data. And while doing any operation with the data, it is mandatory to clean it and put it in a formatted way. So, for that, we use the task of data preprocessing. Real-world data generally contains noise, missing values, and perhaps in an unusable format that cannot be used directly for machine learning models. Data preprocessing is a necessary task to clean the data and make it suitable for a ML model which also increases the accuracy and efficiency of a machine learning model.
Note that this \emph{data preparation} may be one of the most difficult steps for any project. The reason is that each dataset is different and highly specific to the project. Nevertheless, there are enough commonalities across predictive modeling projects that we can define a loose sequence of steps and subtasks that you are likely to perform:
\begin{itemize}
    \item Getting the dataset
    \item Importing libraries
    \item Importing datasets
    \item Finding Missing Data
    \item Encoding Categorical Data
    \item Splitting dataset into training and test set
    \item Feature scaling
\end{itemize}
Following key steps I implemented in my work are shown: using Pycharm (Python IDE) I built a new project (called \emph{activity\_recognition\_keras\_tf2}), using Tensorflow 2.0 with the following folders: \emph{dataset\_tools} (preprocessing tools), \emph{cnn\_estimator}, \emph{lstm\_estimator} and \emph{docs}. Each one contains these folders too: \emph{config} (with the configuration file), \emph{docs}, \emph{libs} (with additional methods), \emph{models} (networks, losses...) and \emph{src} (with main scripts). Now, let's see on detail how \emph{dataset\_tools} folder was built up. In particular, running in sequence the two main script in \emph{src} folder it is possible to perform the dataset preprocessing.


\subsection{Dataset parsing}
The term Parsing is generally defined as \emph{"the parsing of an input to organize the data according to the rule of a grammar"}. However, there are a few ways to define parsing, depending on the context in which you use it. The gist remains the same: parsing means finding the underlying structure of the data we are given. \\In a sense, parsing can be considered the inverse of templating: identifying the structure and extracting the data. In templating, on the other hand, we have a structure and fill it with data. In the case of parsing, you have to determine the model from the raw representation, while for templating, you have to combine the data with the model to create the raw representation. Basically, parsing is necessary because different entities need the data to be in different forms. Parsing allows you to transform and model the data in a way that may be required by specific software. The obvious example is programs, written by humans but need to be executed by computers. So, humans write them in a form they can understand, then software transforms them in a way that can be used by a computer. However, parsing may also be needed when passing data between two pieces of software that have different needs (i.e. when you need to serialize or deserialize a class). \\ In my specific case the parsing is performed by $Unity2oid.py$ script, composed by the following steps:
\begin{itemize}
    \item Embeds the Unity raw Dataset (folders have alphanumeric random names);
    \item Check if it is complete (250 frames per each sample):\\ -\emph{if complete}: continue,\\
    -\emph{else}: break and point out the corrupted sample
    \item Rename and sort each sample folder (000000, 000001, 000002, ...);
    \item Convert sample folders to OID dataset (Figure 4.2):\\ each folder is now characterized by RGB + Label (whit BB in file.txt);
    \item Split all in train\_set and val\_set: \\ according to the percentage specified in file config.ini;
    \item Return the complete OID dataset.
\end{itemize}
Of course to do this, several support methods have been implemented from scratch in the \emph{utils} folder. For example it is the case of \emph{converting\_dataset}() method which: (1) creates the output directory if does not exist, (2) iterating through Unity folders (starting from 00000x where x=0, incremented at each \emph{for} step) and into Unity data (json files captures), search for activities file-names and annotations (Bounding Box) and (3) allows to copy and paste original RGB files and annotations putting the latter in txt files into the Label folder. 
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/oid.png}
\caption{Handmade structure of one sample in OID dataset}
\end{figure}\\
A possible visual example of it is shown in Figure 4.3, where we have as current activity \emph{"Hands Up"}; values in the file.txt are BB coordinates, where the first is the BB class name, and follow respectively rectangle coordinates (x, y, x+width, y+height).\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.55]{Images/oid-sample.png}
\caption{Extracted single frame from an OID dataset sample}
\end{figure} \\The dataset contains a total of 2000 samples: 1500 are intended for training, splitted in 80\% train\_set (1200) and 20\% val\_set, while 500 are put into test\_set. Given these huge amount of data, running the \emph{"Unity2oid"} conversion takes a very long time: more than 2 hours for each activity.


\subsection{Preprocessing pipeline}
Once we get the OID dataset with characters bounding boxes for each activity frame, some other preprocessing step needs to be performed right now. Indeed, the goal now will be to transform the dataset again and make it suitable for Activity Recognition. This additional preprocessing pipeline can be executed by running the script \emph{adapt\_data\_for\_training.py}. It is composed by the following points:
\begin{itemize}
    \item Given bounding boxes coordinates, crop each frame 
    \item Frame sampling according to config file (set to 50)
    \item Retrieve human joints using the pretrained Skeleton Estimation network
    \item For each frame, save each joint set in h5 files
    \item Return the final ar\_dataset
\end{itemize}
So the ar\_dataset (Figure 4.4) includes for each activity sample (1) folder containing original but cropped images (250), (2) folder with sampled cropped images (50) and (3) folder with skeleton joints collected in a h5 file. Number of samples is the same described in the previous section (1200 train, 300 val, 500 test). 
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.4]{Images/ardata.png}
\caption{Abstract example of how ar\_dataset train\_set is organized}
\end{figure}\\
The script adapt\_data\_for\_training.py can make this thanks to the several support methods written in utils folder. For example there is the \emph{store\_cropped\_images}() method, which can read the BB class name and  crop the original image according to their coordinates (Figure 4.5). This is done to facilitate the skeleton estimator in order to provide a single person as input because if there were more people, as we have already said, the model would not know on which character to focus. \\\\\\ Another utility method is \emph{filter\_and\_store\_joints}() which perform a frame down sampling (in case we set the flag $sampling=true$ in file config.ini) and save all the numpy arrays containing joints into h5 file: the name of each file is joint\_list.h5 and it is the same for each sample and activity. Finally, it is important to note that dozens of other scripts have been created from scratch to obtain small functions such as "extract from skeleton estimation only the 3D joints among the many outputs that the network gives us", "print the progress bar during skeleton estimation", "get the frame offset for a parametric frame sampling", "do image resizing when needed" and so on. 
Also in this case, this script is very slow, even more then the dataset parsing one: skeleton estimation takes about 40 second per sample, for a total amount of 22h per activity.\\ 
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.31]{Images/handsup-ar.png}\\
(A)\\
\includegraphics[scale=0.31]{Images/falling-ar.png}\\
(B)\\
\includegraphics[scale=0.31]{Images/wave-ar.png}\\
(C)
\caption{Cropped images coming from several different samples in ar\_dataset. \\ Just 3 frame showing our activities (A) Hands Up, (B) Falling, (C) Waving: note how each image dimension changes according to the current activity, camera and randomization.}
\end{figure} \\ Remember for each activity sample, the collected joint arrays have now this shape: 50x19x3 where 50 is the frame number, 19x3 the number of the extracted 3D skeleton joints (57). See chapter 3.3.2 for more examples about them. 
\newpage
\subsection{Filtering real dataset exploiting YOLO person detection}
We have already seen Unity allows the generation of custom synthetic datasets, for each activity that we need, providing not only a non-repetitive and qualitative dataset, but has many tools for the extraction of labels such as bounding boxes around the person we want to observe. So Unity relieves us from a task that today is seen as a challenge in the world of Computer Vision: the $labeling$. Actually we ask ourselves, what do we do if we deal with a real dataset, with real people, where Unity tools can no longer act? The easiest way, but less logical in terms of time and effort, is obviously the hand labeling: frame the object of interest and click! It is not very elegant, but it is the only way to do it when there are no available tools which perform it. For example, it happens when we want to detect uncommon objects, on which (maybe) no previous work has been done, or there is a lot of noise in the images recorded by the cameras (and so on...). In some cases, however (as in our) there are visual tools that allow you to perform labeling in a totally automatic way: this is the case of the Object Detection. At the State Of The Art today there are several usable and completely open-source networks such as: R-CNN, Fast R-CNN, Faster R-CNN and YOLO. These powerful tools are the answer to our previous question because they allow us to perform very well the person detection we need, catching bounding boxes in the dataset images. YOLO (version 4) was the key tool to carry out my personal approach with real domains (an entire description of it is provided in the last section of this chapter.\\\\\\
\textbf{Retrieving real datasets}\\ As we have already seen in the previous chapters, working on synthetic data has a lot of benefits; however, as it happens in the whole ML world, we must move to the real domain sooner or later and test our work on real dataset. In this work I did this, using 2 real (single person) dataset:
\begin{itemize}
    \item Custom dataset
    \item UT-Kinect Action3D dataset
\end{itemize}
The first (Figure 4.6) was collected by myself both in Konica Minolta laboratory (with indoor $Mobotix$ cameras) and at home with smartphone. I performed the three activities trying to replicate very similar movements of the synthetic domain: I collected 10 videos for each activity.\\ The second (Figure 4.7) is a SOTA dataset, collected for a research project: \emph{"View Invariant Human Action Recognition Using Histograms of 3D Joints"} (2012). Despite this dataset was thinked and collected for depth sequences segmentation, in the course of the years has been taken like example in order to test the own searches on the action/activity recognition from many other successive projects. It presents 10 subjects performing twice 10 different actions (with frame rate equal to 30fps): walk, sit down, stand up, pick up, carry, throw, push, pull, waving hands, clap hands. So we have a total of 200 samples, but between these activities only the 20 samples about waving hands can be used for our trained model (the other activities are obviously unknown). 
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.4]{Images/handsup-custom.png}\\
(A)\\
\includegraphics[scale=0.39]{Images/fall-custom.png}\\
(B)\\
\includegraphics[scale=0.4]{Images/waving-custom.png}\\
(C)
\caption{Custom dataset examples: (A) Hands Up, (B) Falling, (C) Waving.}
\end{figure}
\\ \\ \\ 
\textbf{Preprocessing a real domain}\\
Exactly like the synthetic dataset, preprocessing is characterized by two main phases:
\begin{itemize}
    \item Sampling real videos and raw datasaet parsing
    \item Joint extraction with Skeleton Estimation
\end{itemize}
To perform the second phase at the same way it is done with synthetic data (running the \emph{adapt\_data\_for\_training.py} script), we need to manage real data in a different way than before, changing radically the first phase.\\
Real data comes from videos: so the first thing to do is sample these and extract the number of frame we need: remember that our trained model are based on 50 frame. With OpenCV tools this is possible, thanks to $cv2.VideoCapture$() which inside a custom script called $sampling\_real\_videos.py$ can return the number of frame you set. Now we need to organize this raw data in a OID dataset, as we did for synthetic domain: to do this we have to get bounding boxes coordinates for each sequence of activity frame. This task was achieved thanks to YOLO-v4 person detection pretrained on COCO dataset (details are in Chapter 4.4.1), implemented in a script provided to me by Konica Minolta. 
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.45]{Images/waving-utk.png}
\caption{UT-Kinect Action3D dataset example for waving hands}
\end{figure}
\\
Following some personal contribution to this code is shown, performed to achieve my needs: 
\begin{itemize}
    \item custom filter to detect only people \\(standard version detect all recognized objects);
    \item custom filter to detect people holding baseball-bats\\ (for future possible work: dangerous situation);
    \item method which retrieves bounding boxes coordinates (and save in Label folder, similar to Unity2oid methods)
    \item method which save all in a (real) OID dataset
\end{itemize}
To run this code we need to use Docker engine too, because by uploding a specific container, it is possible to easily start Tensorflow-Serving, the server to which the Konica Minolta team exported the pre-trained YOLO model. Once the Docker container is running we can easily launch YOLO-v4 and make inference on raw images (Note that a small introduction to Docker is locate in section 4.4).\\\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.46]{Images/yolo-me.png}\\
(A)\\
\includegraphics[scale=0.46]{Images/yolo-utk.png}\\
(B)\\
\caption{Capturing bounding boxes with YOLO-v4.}\\
\end{figure}

Setting the YOLO threshold to 0.7 this script is able to capture only the bounding boxes we are interested for, with an overall time of inference of about 18 seconds for sample (50 frames, each one detected in $\sim$ 0.35 seconds). Figure 4.8 shows how YOLO works over samples of both datasets. Note in (B) how two bb are detected on the same image: thanks to the high threshold only the labels with more confidence is taken and saved in Label folder. In this way we can remedy one of the biggest problems in real environments, which (obviously) with Unity never arises. \\ Once we have the OID dataset it is very easy to run the same script we used for synthetic domain (\emph{adapt\_data\_for\_training.py}) and generate the final ar\_dataset (\emph{test\_set}). Internal structure of each dataset is the same we already seen in the previous section and the number of joints will have the following shape: [50x19x3] multiplied by the number of current samples.\\
In Figure 4.9, cropped samples coming from ar\_dataset are shown:
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.4]{Images/cropped-real.png}
\includegraphics[scale=0.42]{Images/cropped-real2.png}
\caption{Samples coming from the 2 different ar\_dataset test\_sets.}
\end{figure}\\ Also in this case (as for synthetic dataset) we can see how the shape of each image changes, according to the movement of the subject. As we already said, the strength of the Skeleton Estimator model is that, even if some part of the body is cut off, it can still estimate the approximate position of the missing joints. This is critical to always get the sequence of joints [50x57] as output along with the images shown above (Figure 4.9). \\ 
In the end it is shown how the AR architecture (including the preprocessing phase) changes and behaves with universal datasets (both synthetic and real).\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/high_architecture_universal.png}
\caption{High AR architecture with preprocessing for every datasets.}
\end{figure}\\
We can see (Figure 4.10) how the input data is preprocessed in the first phase with tf-serving running on Docker engine to guarantee YOLO-v4 model performing the person detection. Data then passes through the \emph{joint extractor} that prepares data for the Activity Recognition engine, which will be the same we already seen in previous sections.
\newpage
\section{Custom architectures}
In this section the main steps of the networks building are described. I implemented each model from scratch but obviously taking as reference past works and networks used in HAR field. In this way I had the opportunity to make much more experience with python and tensorflow libraries, like keras which is entirely dedicated to Machine Learning tools. With a very few lines of code it is possible to carry out (reproducing or building from scratch) every network we want. In my work I decided to implement both a CNN and a LSTM network; while the latter has a very simple structure taken from \url{https://machinelearningmastery.com} only to get a direct comparison, the CNN is my workhorse and I will present nex how it was built. It must be said that, in this case, I did not invent it but as Machine Learning engineers do, I carried out a deep scouting of the networks that deal with the State Of The Art HAR and I took one of these as an example; obviously not having available its code I re-implemented it with Tensorflow-v2 from scratch, implementing some changes over time. The reference paper with original architecture is \emph{"Human activity recognition based on optimal skeleton joints using convolutional neural network"}. Having obtained excellent results with this model following a classic DL approach, then I chose to implement the Continual Learning techniques on board.\\
Following we will see how both networks were built.
\subsection{CNN}
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/cnn.png}
\caption{Custom CNN architecture. \\
The input is referred to the train\_set (with 1200 samples)}
\end{figure}
In Figure 4.11 is shown in detail the structure of the CNN implemented for this work of thesis: the input shape is a falatten RGB-D vector where 1200 is the number of the samples in the train\_set (so can vary), 50 is the number of sampled frames, 57 is the flatten vector of 3D skeleton extracted joints and 1 is the depth channel (empty, added just for shape completeness). My network has 2 Convolutional Layers, 2 pooling layers, 2 layers for batch normalization and 3 fully connected layers. For the first part of the network, the 2 convolutional layers presents respectively a number of filters equal to 32 and 64, with kernel sizes set to (3,10) and (5,5). Then 2 max-pooling with kernel size=(2,2) are used, the last pooling layer has been choosed to perform an average of the data available, so we have an avg-pooling before the input of the first fully connected layer. Each max-pooling layer is followed by batch normalization (avoid overfitting and stabilize the network). Then a flatteing layer is added to compact the entire pooled feature map towards the final part of the network, that is characterized by 3 FC layers: initially only two were thrown, but then a third was added, slowing down the network but guaranteeing it strength in managing the relationships between the different classes with the aim to generalize the concept of activity. A ReLU activation function is used inside all the CNN except for the last fully connected layer in which a softmax activation function is used. We substancially induce the network to predict the input tensor with only positive values (normalized) to obtain an implementation consistent with the ground truth batches. In this way, excluding the possibility of negative values inside the cells, the loss function will encounters all the predicted values from the network in the range between 0 and 1, the same range that represents the minimum and maximum distances of the normalized skeleton joints. \\Everything is done with methods and script written completely from scratch, batch generation and data extraction too: the script $gt\_btc\_gen.py$ takes as input path the directory of ar\_dataset and then iterating through its sub folders (train and val set) fills x\_data and y\_data thanks to \emph{generate\_train\_val\_batch}() method. Note that according to the current flag it is possible to extract from this data both values for train\_set and val\_set. This method iterates through specified activities (expressed in file config.ini) and for each one it loads its joint\_data (h5 file saved during the preprocessing pipeline). Then we can extract the first tensor for x\_data=(1, 57) where 57 is the flattened skeleton\_data (19x3) and we fill this $numpy$ array of zeros with \emph{flatten}() operation; the first term will be incremented and filled until reach 50 (number of frames). Then we extract the first tensor for y\_data=(1, A) where A is the max number of allowed activities and we set the labels: in our specific case A would be equal to 3 for classical DL approach so we have $[1, 0, 0]$ for manDown, $[0, 1, 0]$ for handsUp, $[0, 0, 1]$ for wavingHands. However in Continual Learning the aim is to exploit the maximum number of network's neurons and to guarantee scalability for subsequent activities that can be added in the future, so we fix A=1000 to leave the network open to new knowledge.\\ The excellent results achieved with this (quite simple) model, shown in the next chapter, demonstrate that it is not always necessary to implement huge networks containing many hidden layers to achieve goals.\\
\subsection{LSTM}
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.38]{Images/lstm.png}
\caption{Custom LSTM architecture. \\
The input is referred to the train\_set (with 1200 samples)}
\end{figure}
In Figure 4.12 is shown in detail the structure of the LSTM I used: the input shape is a numpy vector which differs from the CNN input vector for this reason (see Chapter 3). Note that this time we don't need the fourth element which depicted depth channel in previous section, but we just deal with the [1200x50x57] vector expressed by sequence of joints over time. I defined a model having a single LSTM hidden layer, containing 100 cells; this is followed by a dropout layer intended to reduce overfitting of the model to the training data. Finally a dense fully connected layer is used to interpret the features extracted by the LSTM hidden layer before the final output layer, used to make predictions.\\ The single LSTM hidden layer holds as activation function a Tanh: so the input data was normalized between the range [-1,1] to guarantee compatibility with the input network requirements. As for the CNN normalization was performed with custom methods written from scratch: I used $normalize\_dataset$() function that take min-max range for the 57-vector containing joints in x\_data shape. By computing this range it was easy to normalize this elements dividing negative values for abs(min\_val) and positive values for abs(max\_val), where abs stands for absolute value. Furthermore, as for CNN data\_batch\_generator was performed. Implementing all the code in parametric way I could use the same scripts and functions already implemented and used. Remember that all this was possible, again, using the Tensorflow-v2 libraries.\\

\subsection{Algorithms and hyperparameters}
During my work with neural networks, I chose to hold some key training criteria and algorithm, such as optimizer and loss, in order to see the effects on different networks (LSTM vs CNN) and approaches (DL vs CL), except for some hyperparameter (training epochs, batch size...) which depending on the current approach needed to be tuned. For instance once you found some factors which guarantee an acceptable stability in the classic DL training, the CL approach introduces new factors to consider: classic tuning will still be needed (especially for epochs and learning rate), but we will be focused on some other kind of hyperparameter, as we'll see in the next section. So I fixed for both training networks the following:
\begin{itemize}
    \item Optimizer: \emph{Adam}
    \item Loss Function: \emph{Categorical Cross Entropy}
\end{itemize}
The choice of optimization algorithm for your deep learning model can mean the difference between good results in minutes, hours, and days. The Adam optimization algorithm is an extension to stochastic gradient descent (update network weights iterative based in training data) that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. The stochastic gradient descent maintains a single learning rate (called alpha) for all weight updates and the learning rate does not change during training; a learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds. This method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. \\The optimizer is implemented in the training pipeline in the same way for both networks, using the Tensorflow libraries (keras); regarding its configuration hyperparameters I fixed $alphae=0.001$, $beta\_1=0.9$, $beta_2=0.999$ and $epsilon=1e-07$, where alpha is the learning rate (proportion that networks weights are updated), beta\_1 is the exponential decay rate for the first moment estimates, beta\_2 is the exponential decay rate for the second-moment estimates and epsilon is a very small number to prevent any division by zero in the implementation. Furthermore the learning rate decay technique was set to False and not used. But what are optimization algorithms really for?\\ The objective of the DL is only one: to generate a model capable of generalizing, then make sure that it is not overfitting, and with a high accuracy, that is, that has the smallest possible difference between the predicted and actual values. To obtain the discrepancy (Delta) between predicted and actual values we use a loss function measured on each observation, which allows us to calculate the cost function. However, we must minimize the cost function by identifying the optimized values for each weight. Here come into play the optimization algorithms. Through multiple iterations these algorithms allow the identification of weights that minimize the cost function.\\
Speaking about loss functions, it is important to introduce the one used in this work: Cross Entropy loss (or log loss). It measures the performance of a classification model whose output is a probability value between 0 and 1 (Figure 4.13). Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.4]{Images/cross-entropy.png}
\caption{Cross Entropy Loss: it shows the range of possible loss values given a true observation}
\end{figure}
Our HAR task is definitely rejoinable to a multiclass classification, since we have activity\_number>2. This must be specified in the code, since in Tensorflow-keras there are two different Cross Entropy loss functions specific to the binary or muticlass case: we need to implement the \emph{Categorical Cross Entropy}.
This function calculates the loss by computing the following sum:
\begin{equation}
    Loss = - \sum_{i=1}^{output\_size} y_i log \hat{y}_i
\end{equation}
where $\hat{y}_i$ is the $i$-th scalar value in the model output, $y_i$ is the corresponding target value and $output\_size$ is the number of scalar values in the model output. \\ This loss is a very good measure of how distinguishable two discrete probability distributions are from each other: $y_i$ is the probability that event $i$ occurs and the sum of all $y_i$ is 1, meaning that exactly one event may occur. The minus sign ensures that the loss gets smaller when the distributions get closer to each other.
\newpage


\section{Elastc Weight Consolidation development}
The Elastich Weight Consolidation is one of the best performing Continuous Learning techniques at the State Of The Art. In Chapter 3 we have already seen its internal mechanism and functioning. In this chapter we will see how this important and innovative methodology was implemented by me for my thesis work, with the goal of learn in continuous 3 activities (Waving Hands, Hands Up, Falling Down) without any forgetting.\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.5]{Images/ewc_space.png}
\caption{An illustration of the concept of Elastic Weights Consolidation.}
\end{figure}\\
Given the first trained model we want to learn the new activity, avoiding the well-known phenomena of \emph{Catastrophic Forgetting}. In Figure 4.14 an illustration of the EWC \emph{task-space intersection} goal is shown: the surfaces of the loss functions $L_A$ and $L_B$ of neural networks A and B are approximated by paraboloids in the vicinity of optimal weights $\theta_A$ and $\theta_B$. The minimum value for the loss along the intersection of two paraboloids corresponds to the weight of the fused network $\theta_F$. \\ \\ During my work I tried to replicate with code all the EWC principles and key points, not writing all from scratch but following several Konica Minolta tips and guidelines to deal with this difficult methodology.\\ According to EWC we can define the following implementation steps, given N different activities:
\begin{itemize}
    \item Train the first activity and return the hdf5 model;
    \item Compute its Fisher matrix;
    \item Train the previous model over the second activity data batch;
    \item \emph{"Lambda"} parameters tuning for adversarial losses and Fisher update;
    \item Repeat for N activities until the goal is reached.
\end{itemize}
In the next section we will see all these steps on detail, up to the development of an \emph{}{automatic pipeline} which can perform the EWC learning for N activities (completely parametric).

\subsection{The Fisher Matrix computation}
The Fisher Information Matrix highlights the weights of the previous tasks and their key features (each related value will be high in the matrix); the sum of the Fishers will take into account all the new information (about trained activities) and it will be multiplied for the quadratic difference between the current model and the one we must no forget (see Chapter 3). Fisher Matrix is used to give an accurate estimate of all the neurons in the network.\\ Let's see how this was performed in the code: in \emph{src} folder I implemented a new script called \emph{compute\_fisher\_matrix.py}; here I upload the validation data with the already presented batch\_generator methods and extract groundtruth (y\_true), converted in tensor. Then I load the trained model (hdf5 file) and compile it again with the Cross Entropy loss. Finally I compute the fisher matrix of the trained model and save it into a npy file. \\ The script \emph{compute\_fisher\_matrix.py} is organized as following: 
\begin{itemize}
    \item Load model weights and initialize the FM as \emph{numpy} vector of zeros;
    \item Compute gradients* for each sample of the network;
    \item Each gradient is returned into the \emph{ret};
    variable;
    \item Compute the maximum value and divide w.r.t. the batch length;\\
    (The max value is implemented to normalize Fisher values between [0, 1])
    \item Return the Fisher Matrix as \emph{numpy} array (Figure 4.15).
\end{itemize}
Gradients* are computed with \emph{compute\_gradients}() method: we compute the distance between what I expect and what I predict; then I calculate the gradient for each layer (weights) and see when the training acceleration vector has moved with respect to the validation set, i.e. how much the weights have moved w.r.t. layers. Essentially, we compute the loss between predictions and groundtruth over a validation set of 300 samples, coming from ar\_dataset. Remember that the first derivative is equal to the velocity vector of the model, but being a function with linear Gaussian probability it is also equivalent to approximating the second derivative of this (which would be by definition the Fisher matrix). Note that for periodic losses we should have calculated the second derivative too (with stronger computational effort). So in the code we get \emph{grad\_list} that will be the list containing all derived layers of the model (it is then squared because we need the mean square value). Note that the Fisher matrix computation could be replaced also by the identity matrix taking all the info (main diagonal), but obviously reporting also all the noise of that model, losing then in terms of accuracy.
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.4]{Images/fisher-inf.png}
\caption{Plot example Fisher Matrix computation in my work.}
\end{figure}

\subsection{The Regularization Loss}
As mentioned in Chapter 3, Catastrophic Forgetting is prevented thanks to the development of two adversarial losses, that during the training of a new class, if well balanced, they succeed to make the model learn the new one without forgetting the previous. \\ This two losses are:
\begin{itemize}
    \item Cross Entropy Loss (which we will call custom loss)
    \item Regularization Loss
\end{itemize}
So following theory formulas (see Equation 3.27), for each weight layer I compute the sum of the squared norm difference between the actual weigth and the regularization term: the overall loss will be given by the sum of cross\_entropy and the reg\_term.\\ In my Pycharm project I defined a new python script called \emph{loss.py} with a new class called Losses that instantiates as self-parameters: in particular we can see here fisher\_matrix, current\_model, regularization\_model, lambda\_1 and lambda\_2; all these variables are initially empty, but then filled during the various training with the respective values. Note two variable not mentioned before among these, the lambda terms: they are constants defined between [0,1] multiplying respectively custom loss and regularization loss. So they can be seen as monitoring factors for the two losses and therefore of main importance when tuning the parameters: recent State-of-the-Art results have shown that the ratio lambda\_1 : lambda\_2 should be approximately equal to $1{^-4}$ (Figure 4.17). A too high ratio could lead the network to forget because the regularization loss would be too small, overpowered by the new cross entropy loss. A small ratio instead does not allow to learn the new task: the regularization loss is so strong that it keeps the weights of the network on the previously learned model. The Lambda hyperparametrization tuning is today a challenging task and it would be useful a method that calculates them in automatic way (see next section). \\ Now let's go back to the Losses class, so characterized by \emph{custom\_loss}() and \emph{ewc\_loss}() methods, both defined from scratch. In \emph{custom\_loss}() is just called the tensorflow-keras categorical cross entropy loss, changing the signature and the return according to our needs. In \emph{ewc\_loss}() is instead implemented the regularization loss, given the custom loss, the actual weights (of the first trained activity) and the regularization weights (of the new added activity): iterating over actual weights length, we first define regularization weight through a casting function to guarantee a shape matching; then we increase the regularization term just applying the paper formulas. Finally we can compute and return the new cross entropy loss multiplying it to lambda\_1 and the regularization loss with lambda\_2.\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.85]{Images/ewc-train.png}
\caption{Example of loss behaviour at training start. }
\end{figure}\\\\
In Figure 4.16 we can see plots of losses value over epochs and each weight: note how already iterating on the first layers, the overall ar\_loss decreases a lot: the goal will be a value close to zero, avoiding to go up again (without learning anything).
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/adv-loss.jpeg}
\caption{Ending ideal behaviour of well-tuned adversarial losses}
\end{figure}

\subsection{Automated training pipeline}
Once implemented every supporting method to perform EWC in python, it is time to write a dedicated training pipeline. During my work I first build from scratch a \emph{"manual"} pipeline (\emph{EWC\_train.py} script) and then I made it \emph{automatic} and parametric for N classes (\emph{automated\_pipeline.py}).\\ The first one was organized with the purpose of getting to the goal explicitly and to make the first tests with Continual Learning: in fact this one only works by adding one class at a time, specifying path and class name, calculating the Fisher Matrix with the related script and finally updating the count variable to remember which batch of values for training to extract (based on the current class). The manual script can be resumed as following:
\begin{itemize}
    \item Load the trained model with its fisher matrix;
    \item Extract the current activity batch from dataset and execute shuffle;
    \item Freeze convolutional layers from the 2nd training (optional)(*');
    \item Set current and regularization models updating losses;
    \item Set lambda values from file config.ini;
    \item Train the new model and compute its fisher matrix (weighted for the third class)(*'');
    \item Repeat same steps for all new classes, just changing paths and count-step.
\end{itemize}
\newpage As we can see, these are the same key steps described in the previous section, except for two points: (*') the possibility to freeze the conv-layers is implemented. This can be done to achieve a different result performing a \emph{"regularized transfer learning in continuous"}, just exploiting the FC layers of the network and all the neurons available; such an approach in the classic DL would immediately saturate the FCs and the network could no longer learn. (*'') An additional method for Fisher Matrix computation is built to keep track not only of the latest model towed, but also of those before. For instance when we want to learn a third activity we need to perform the following calculation:
\begin{equation}
FM = 0.3 \times (fisher\_all\_previous\_tasks) + 0.7 \times (last\_computed\_fisher) 
\end{equation}
Once the first results were achieved, all this was evolved, following the advice of my supervisors in Konica Minolta, into a fully automatic and parametric training script for N classes. \\This \emph{automated pipeline} is done essentially to not push run every time when we want to train multiple classes in sequence (and then it's the goal of engineers to automate everything!). In particular \emph{automated\_pipeline.py} repeat all the above steps (implemented in \emph{EWC\_train.py}) but taking into account an initial count\_step can carry out the EWC task for all the classes in the dataset. This is done with a very big for cycle that iterates between the classes by increasing each time the count\_step (initialized to 1 at the first training). So it performs the following steps: (1) start from the first activity (Waving Hands) and train it (single-activity training) and compute its Fisher Matrix. (2) Then load this model (count\_step=2), extract the current batch data, update Losses models (current and reg) and Fisher Matrix and (3) start new training, setting lambdas (tuning) to converge. (4) To compute the Fisher matrix this time take the weighted average (as discussed above) and shift to the third activity (count\_step=3), (5) repeat all the above steps for N classes.\\ \\ Both during the writing and running of this pipeline, several problems have arisen: while some have been resolved at the implementation level, for others it has been necessary to change even strategy as we will see below. In fact, after a long time \emph{"passed talking to a duck} (Figure 4.18), I managed to find a solution to them.\\
- The first faced problem, although it seems a blunder, was with a compilation error due to the checkpoint used in the automated pipeline, when I tried to compile and train the new updated models in the same loop. My patience was tested here because I couldn't find any solutions on the web; the reason for this became soon clear when I found out that the error was due to overlapping python pointers. So this is solved writing an utility method $compile\_neural\_network\_model$() when compiling, with the following steps: A) initialize the model with the custom\_cnn each time to avoid pointers overlapping; B) compile with the current loss and return the model, updated this time and ready for training.\\
- The second faced problem was the difficulty to find training stability assigning values to lambda (hyperparameters tuning); respect to the manual pipeline, here the internal values (of Fisher Matrix, activity training...) change every time we run the engine. Before we just run and save models, now we retrain from scratch the entire engine. So for example, if after the hyperparameters tuning we find good results, it is not certain that those values (of lambda especially) will fit well the second time. The only way to avoid this problem could be implement an automatic hyperparametrization of lambda terms, upsetting the State Of The Art, because there is nothing about it at the moment. During my apprenticeship I have been thinking how to do this, developing a new methodology that can reach that goal, but it is still in progress, for a future possible \emph{publication} in collaboration with Konica Minolta and Sapienza.\\
- The third arisen problem was the biggest one: carrying out the first experiments we realized that the activities generated were too few to obtain, with the parameters we were using, immediately good results. For example, performing a \emph{"single-activity"} will confuse the Fisher Matrix computation due to strong overfitting, returning not always good values. Furthermore very similar activities like Hands Up and Waving Hands can confuse the network to classify activities. During training, the optimizer runs too fast, going from learning nothing to forgetting everything in a few epochs. Since the generation/preprocessing time of new tasks with Unity would have been too large, it was decided to proceed in the following way, working in the range of the optimization and stretching the training:
\begin{itemize}
    \item Reduce the Learning Rate ($10^{-4}$, $10^{-5}$);
    \item Augment the Batch Size (from 32 to 64, 128);
    \item Increase the number of total Epochs, but saving internal checkpoints to estimate how the optimizer behaves;
    \item Reduce the samples coming from dataset.
    \item Set lambda\_1 = 1 (fixed)
    \item Set lambda\_2 $\in$ [$10^{-10}$, $10^{-5}$]
\end{itemize}
In this way I changed the optimizer range and the way it run, slowing it down a lot. Another reason for this found instability is due to the fact that in SOTA approaches, in general CL is applied in a different way than I did: (1) take 2 to N classes and train them in the classical way, computing a Fisher Matrix (which this time will be able to properly extract the information it needs) returning the model; (2) perform CL by adding one action at a time (with EWC approach). So what stops us, as mentioned earlier, is the lack of data and classes. \\Despite this, I still achieved interesting results in both synthetic and real domain (as we will see in the next last chapter) which makes the described approach valid.
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.4]{Images/ducky.jpg}
\caption{Rubber Duck Debugging: a very common practice among engineers, to tell a rubber duck about the code problems you are experiencing so that you can think through and solve those problems.}
\end{figure}
\newpage

\section{Frameworks and software involved}
This concluding paragraph discusses about three very common tools in deep learning which have proved fundamental to my work: Tensorflow, Docker and YOLO. While Docker and YOLO were only runned to enable the person detection I used with real datasets, Tensorflow is an integral part of all code developed in python: Tensorflow is an open-source software library (framework), developed by the Google Brain team for internal Google use and was released under the Apache 2.0 open source license on November 9, 2015. TensorFlow provides official Python and C APIs and also without guarantee of stability APIs: C++, Go and Java. Its flexible architecture which allows for the easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices. In fact, it enables the use of CUDA GPU computation (Compute Unified Device Architecture) to process over millions of images per day, which is why Tensorflow is also suitable for industry and internet-scale media. CUDA is a recent technology provided by NVIDIA that enables the use of Nvidia graphics cards for parallel computing (Figure 4.19). The GPU is then used to process code instructions, specifically a large number of cores, i.e. Cuda cores, are made available to augment the parallelization process in order to optimize the cost of computation. Anyone who wants to take advantage of this resource must have at least one Nvidia video card equipped with a GPU that meets certain requirements that generally differ from the type of hardware architecture of the card itself, the higher the cost and quality of the video card on the market, the more cores will be available for optimization. Although the first available programming languages were written in C and without recursion or memory pointers, Nvidia has been able to extend its software with the most up-to-date languages including: C++, Python, Java; this new scenario has literally made GPUs true open architectures, CPUs are able to offer enormous performance to all applications compatible with them, an example is the CUDNN library, or CUDA Deep Neural Network library, composed of a set of primitives for deep neural networks for standard routines such as normalization, forward/backward convolution, activation layers, pooling (...). \\ In my specific case I installed CUDA-v11.0.3 and CUDNN-v8.0.2 with the following simple steps:
\begin{itemize}
    \item Download CUDA with $wget$ from website;
    \item Install it by running $sudo$ $sh$ $file.run$;
    \item Add directories in the laptop bashrc digiting $echo$ $>>$ $\sim$ $bashrc$ (or just opening it with gedit tool);
    \item Download the tgz CUDNN file for Ubuntu and unzip with $tar$;
    \item Copy file paths into laptop with $cp$ command;
    \item Install it, remembering the $chmod$ $a+r$ command.
\end{itemize} 
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.5]{Images/cuda.png}
\caption{Parallel computing platform and programming model developed by NVIDIA for general computing on GPUs}
\end{figure} 
Finally, Tensorflow presents a deep learning library in python called Keras that can be run for quick and practical experimentation in a very simple way. \\ Docker, on the other hand, is a software platform that allows you to quickly create, test and deploy applications. Docker collects software in standardized units called containers that offer everything necessary for their proper execution, including libraries, system tools, code and runtime. With Docker, you can deploy and recalibrate the resources for an application in any environment, while always keeping an eye on the executed code. In my work I installed it with the following command line: $sudo$ $apt-get$ $install$ $-y$ $nvidia-docker2$ \\Docker implements high-level APIs to manage containers that run processes in isolated environments. As it uses Linux kernel functionality (mainly cgroup and namespaces) a Docker container, unlike a virtual machine, does not include a separate operating system. Instead, it uses kernel functionality and leverages resource isolation (CPU, memory, I/O, network) and separate namespaces to isolate what the application can see in the operating system. Docker accesses the virtualization capabilities of the Linux kernel either directly using the libcontainer library, or indirectly through libvirt, LXC, or systemd-nspawn tools. \\ Using containers resources can be isolated, services restricted and processes started so that they have a completely private perspective of the operating system, with their own identifier, file system and network interface. Multiple containers share the same kernel, but each may be forced to use a certain amount of resources such as CPU, memory and I/O. Using Docker to create and manage containers can simplify the creation of distributed systems, allowing different applications or processes to work independently on the same physical machine or on different virtual machines. This allows new nodes to be deployed only when needed. \\ Now I think it's important to present and talk about the YOLO engine, so that a separate section is dedicated to it.
\\ \\
\subsection{YOLO-v4}.\\ YOLO (You Only Look Once) is an extremely fast CNN objects detector. Unlike a neural network that predicts bounding boxes and class probabilities directly from the entire image into a single evaluation, YOLO detects objects as a regression problem to spatially separated bounding boxes and corresponding class probabilities. Since the image processing is simple and straightforward this network can be trained directly on the complete images. The complete structure of the network is shown in Figure 4.20.
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.6]{Images/yolo-struct.png}
\caption{YOLO architecture.}
\end{figure}
YOLO has 24 convolutional layers followed by 2 fully connected layers. Alternating 1 × 1 convolutional layers reduce the features space from precedenting layers. \\ \\ \\ The activation function (AF), illustrated in the previous chapter, is linear for the last layer. All other layers use the following leaky rectified linear activation as:
\begin{equation} 
\phi(x)=
\begin{aligned}
\begin{cases}
    x  & \quad \text{if } x \text{ $>$ } 0\\ 
    0.1x  & \quad \text{otherwise }
  \end{cases}
\end{aligned}
\end{equation}
and the multi-part loss function using during the train is of the form:
\begin{equation}
\begin{aligned}
loss &= \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathfrak{1}_{ij}^{obj}\big[\big(x_i - \hat{x_i}\big)^2 + \big(y_i - \hat{y_i}\big)^2\big] \\
&+ \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathfrak{1}_{ij}^{obj}\bigg[\bigg(\sqrt{w_i} - \sqrt{\hat{w_i}}\bigg)^2 + \bigg(\sqrt{h_i} - \sqrt{\hat{h_i}}\bigg)^2 \bigg] \\
&+ \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathfrak{1}_{ij}^{obj} \Big(C_i - \hat{C_i}\Big)^2 \\
&+ \lambda_{noobj} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathfrak{1}_{ij}^{noobj} \Big(C_i - \hat{C_i}\Big)^2 \\
&+ \sum_{i=0}^{S^2} \mathfrak{1}_{ij}^{obj} \sum_{c \in classes}^{}  \big(p_i(c) - \hat{p_i}(c)\big)^2 
\end{aligned}
\end{equation}
where $\mathfrak{1}_{i}^{obj}$ denotes if the object appears in grid cell $i$ and $\mathfrak{1}_{ij}^{obj}$ denotes that the responsible for that prediction is the $j$th bounding box predictor in grid cell $i$.
Following the necessary steps to understand how YOLO predicts the output bounding boxes starting from the input image are described. The first step splits the input image into an S x S grid in which each cell predicts $B$ 
bounding boxes and a scores corresponding to how confident the model is about the box contain also how accurate the box is, as depicted on the top image on Figure 4.21.\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.51]{Images/yolo-v4.png}
\caption{YOLO model.\\ YOLO splits the image into an 13 x 13 grid and for each grid cell predicts B bounding boxes, confidence for those boxes, and C class probabilities.}
\end{figure}\\
These scores are called \emph{confidence scores} and are evaluated as:
\newline
\begin{equation}
Pr(Object)\cdot IOU_{pred}^{truth}
\label{eq:confidence_scores}
\end{equation}
\newline
Intersection over Union (IOU) is a metric evaluation used to measure the accuracy of an object detector on a particular dataset (any algorithm that provides predicted bounding boxes as output can be evaluated
using this evaluation). From the Equation \ref{eq:confidence_scores} it is clear that for all objects inside the cell the score will be the IOU between the predicted box and the ground truth; in case there are 
no objects in that cell, the result will be zero. Five prediction values like $x,y,w,h$ and the confidence make up each bounding box. The $x,y$ coordinates represent the center of the box relative to the bounds
of the grid cell (at difference of the $w,h$ coordinates that are predicted relative to the entire image). The value of the confidence is relative to the IOU between the predicted box with respect any ground
truth box.
The bottom image in Figure XXXD shows the \emph{Class probability map}, in particular each grid cell predicts $C$ conditional class probabilities $Pr(Class_i | Object)$ influenced by the presence 
of an object. Both confidence scores and C class probabilities are encoded as an $S \times S \times (B \cdot 5 + C)$ tensor, with the result of a class-specific confidence scores for each box as:
\newline
\begin{equation}
Pr(Class_i | Object) \cdot Pr(Object)\cdot IOU_{pred}^{truth} = Pr(Class_i) \cdot IOU_{pred}^{truth}
\end{equation} \\
YOLO (in particular its latest version "$4$") is orders of magnitude faster than other object detection algorithms (Figure 4.22).\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.4]{Images/yolo-speed.png}
\caption{In SOTA experiments YOLO-v4 obtained an AP value of 43.5 percent (65.7 percent AP50) on the MS COCO dataset, and achieved a real-time speed of FPS, beating the fastest and most accurate detectors in terms of both speed and accuracy.}
\end{figure}\\
The limitation of YOLO algorithm is that it struggles with small objects within the image, for example it might have difficulties in detecting a flock of birds. This is due to the spatial constraints of the algorithm.\\
The version I used in my work is the one depicted in Figure 4.22. To make detections, it it pretrained with COCO, a large-scale object detection, segmentation, and captioning dataset: its popularity makes it widely used to benchmark the performance of computer vision methods (Figure 4.23).\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.45]{Images/coco.jpg}
\caption{COCO dataset examples}
\end{figure}


\chapter{Experiments}
\lettrine[lines=2, findent=3pt, nindent=0pt]{I}{}n this chapter, the aim is to show the best experiments conducted in the field of HAR, using both the classical approach with the two constructed networks, and the continuous (EWC) approach. In particular, the models are all trained on the synthetic dataset and then tested on 1)synthetic test set, 2)custom real dataset, 3) an activity extracted from UT-Kinect Action3D Dataset (SOTA). As we saw in the previous chapter, continuous trolling was not an easy task, but I still achieved great results. For each experiment, not only results but also the related hyperparameters are reported.

\section{Synthetic AR dataset accuracy evaluation (Unity)}
In this section only the experiments performed with synthetic dataset are shown. For both approaches I obtained good results, as we will see following, but obviously models are quite \emph{"overfitted"}, because samples in the training dataset are very similar to those in the test\_set. However this does not mean that models have not learned, but only that retrieved accuracy will be a little higher than it really is. A confirmation of this comes then in the following section, where such models are tested in real domains.

\subsection{Classical approaches}
To see the different behaviors and make a peer comparison, both networks was trained at start with the same hyperparameters:
\begin{itemize}
    \item Batch Size = $32$;
    \item Epochs = 15;
    \item Loss = (Categorical) Cross Entropy;
    \item Optimizer = Adam;
    \item Learning Rate = $10^{-3}$;
    \item beta\_1 = $0.9$;
    \item beta\_2 = $0.999$;
    \item epsilon = $10^{-6}$;
    \item activities total number = 3;
    \item train\_set = 1200, val\_set = 300;
    \item test\_set = 500.
\end{itemize}
Note that these parameters derive from an earlier tuning. Also, as we will see below, a few parameters were changed during some experiments.\\
\textbf{CNN}\\
The first experiment with CNN has exactly the above hyperparameters and, as shown in Figure 4.24, the training trend was the following:\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.5]{Images/cnn2.jpeg}
\caption{Training trend plot of the first experiment (realized with python \emph{plotlib}}
\end{figure}\\
As we can see, the validation trend reach a good accuracy exactly after 14 epochs. The model, tested over 500 samples of the test\_set is shown in Figure 5.2.\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.35]{Images/cnn2_cm.jpeg}
\caption{Confusion Matrix of the first experiment. \\The main diagonal indicates the correct predictions on the three activities.}
\end{figure}\\
The final accuracy is about 0.9 and this is the best model I found: in particular, as we will see in real domain testing, this model is not overfitted. \\An example of overfitted model instead comes from the following experiment. \\An apparent good result is derived from that, keeping the above hyperparameters fixed, but reducing just the Batch Size to 16:\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.5]{Images/cnn1.jpeg}
\caption{Training trend plot of the second experiment}
\end{figure}\\
As we can see in Figure 5.3, the model reach high accuracy faster this time, after just 9 epochs. So the trained model can seem very strong (as shown in Figure 5.4) but as we already said, it is fully overfitted (too epochs made adapt it a lot to synthetic data and fake activity mechanisms, so the model will fail easily when movements will differ slightly (like in real domains).\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.35]{Images/cnn1_cm.jpeg}
\caption{Confusion Matrix of the second experiment.}
\end{figure}\\\\
\textbf{LSTM}.\\
Although the Continual Learning was performed through the CNN, I decided to test the classical approach on the HAR also with the created LSTM, by personal curiosity and in order to get other material on which compare the final results.\\
The first experiment was performed again with the same hyperparameters reported at the beginning of the section. The result is the following (Figure 5.5):\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.5]{Images/lstm1.jpeg}
\caption{Training trend for LSTM}
\end{figure}\\
The curve relative to the training set is higher than that relative to the validation set, which makes us immediately think to a remarkable overfitting of the model. In fact, the Confusion Matrix (Figure 5.6) returns what seems to be a very good accuracy (that we will discover how much is "fake" when we will pass to the tests in the real domain).
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.35]{Images/lstm1_cm.jpeg}
\caption{Confusion Matrix for LSTM (first experiment)}
\end{figure}\\
So another experiment was performed, by decreasing the number of epochs (now equal to 5) and the learning rate (equal to $10^{-4}$) too. Figure 5.7 shows the new training trend.\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.5]{Images/lstm2.jpeg}
\caption{Training trend for LSTM (reduced training)}
\end{figure}\\\\\\\\\\
Although we slowed down the optimizer, overfitting remains in this model (accuracy and CM shown in Figure 5.8). This in fact should be fought both with a better tuning of the parameters, or by introducing the classic techniques that avoid it (regularization terms, less hidden units, and so on). However, it is not the goal of this work, since we are not interested in LSTMs; so we will take as good such model.\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.35]{Images/lstm2_cm.jpeg}
\caption{Confusion Matrix for LSTM (second experiment)}
\end{figure}

\newpage
\subsection{Continual learning approach}
To perform EWC methodology I first set the same hyperparameters specified at the beginning of the chapter. Obviously a lot of them have been changed during experiments, due to all the encountered problems I described in Chapter 4.3.3. Moreover, two parameters becomes important here: the lambda terms. As we already said these values can control the losses vemency, letting $lambda\_1 = 1$ and changing $lambda\_2$ each time.\\
\\ \textbf{First training: Waving Hands \& HandsUp.}\\
After the first \emph{single-activity} training with Waving Hands, I trained the model (WH) over data batch of the second new activity (HU) following the EWC steps; I obtained some first results, reducing epochs (Figure 5.9) setting lambda\_2 = $10^{-5}$:\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/cm_cl_failA2.jpg}\\
A\\
\includegraphics[scale=0.3]{Images/cm_cl_failA1.jpg}\\
B
\caption{Failed experiments: (A) the model not learn the new activity (only 1 epoch); (B) the model forgets everything (2 epochs).}
\end{figure}\\
The first thing I noticed was that the optimizer run too speed, so by acting as described in Chapter 4.3 I finally found a good solution. In particular I reduced the Learning Rate to $10^{-4}$ and I augmented the Batch Size to 64. With only one epoch the model can learn the second activity without forget the first (Figure 5.10), with overall accuracy = 0.993.\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.33]{Images/cm_cl_bestA.jpg}
\caption{Confusion Matrix for the best binary model found.}
\end{figure}\\\\\\
\textbf{Second training: adding Falling Down}\\
Using the same hyperparameters of the first EWC training and just changing $lambda\_2$, I obtained some different results leading me to the following possible model behaviours:
\begin{itemize}
    \item The model forget all;
    \item The model don't learn the third activity;
    \item The model learn all activities but with general accuracy decay (Figure 5.11).
\end{itemize}
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/cl_failC.jpg}
\caption{Failed EWC attemps with $lambda\_2 = 10^{-7}$.}
\end{figure}
Our optimizer is once again too fast! \\ So I further reduced the Learning rate to $10^{-5}$, setting $lambda\_2$ to $10^{-10}$. The epochs were increased to 120 to stretch the range too, saving internal checkpoints to see how the model changes and predicts over time. In Figure 5.12 Confusion Matrices are shown, in particular for models extracted at epochs 1, 64 and 100.
\newpage
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.29]{Images/cl_best1.jpg}
Epoch = 1
\includegraphics[scale=0.29]{Images/cl_best2.jpg}
Epoch = 64
\includegraphics[scale=0.29]{Images/cl_best3.jpg}
Epoch = 100
\caption{Confusion Matrix for three different checkpoints}
\end{figure}
\newpage
For each epoch and model checkpoint, then I plot the prediction score (overall accuracy) retrieved to get an estimate of its trend in continuous (Figure 5.13):
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.35]{Images/trend_100_ep.jpeg}
\caption{Predictions trend of models over time with Continual Learning}
\end{figure}\\
So from this we can understand that our Task Space is limited that seems behaving in the shown strange way; this depends by the several reasons I explained in Chapter 4.3 (dataset, similar activities...). In Figure 5.14 we can see a representation of the task space for the three epochs taken as a sample: the point of intersection (see Chapter 3) is very oscillating and not stable, passing immediately from a task to another. The best accuracy I found is at epoch 64, equal to 0.861. It is quite good for our purposes: in this specific contest, there are only two alternative to get higher accuracy: (1) using a bigger dataset with at least 5 activities trained with classic approach in a first moment and then applying EWC technique adding all the activities we want in sequence; (2) working with totally different activities, which never confuse the model training.\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.35]{Images/trend_taskspace.png}
\caption{Task Space for our Continual Learning approach}
\end{figure}
\\\\
Once understood these mechanisms and playing with lambdas, I reached many other results similar to the "best" one shown above (Figure 5.12); in particular, while in the previous case I needed a greater number of epochs, with the found model I needed only 6 to reach a good overall accuracy (0.87). In Figure 5.15 is possible to see the Prediction Trend of this trained model for a total of 12 epochs.
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.35]{Images/trend_few_ep.jpeg}
\caption{Predictions trend: same behaviour of the previous trend}
\end{figure}\\\\
\textbf{Alternative experiment: changing the activity sequence.}\\
Finally another kind of experiment was tried to see if this problem of forgetting the second activity to make room for the third was a problem of Waving Hands and Hands Up. In fact I reversed the order by first performing a "single-activity" training on Falling, adding in sequence first Waving and then Hands Up. Also here, I had to bring to the bone the network using the same techniques used previously. Performing a careful tuning of the parameters, the best accuracy found (0.862) is that achieved at epoch 71 as shown in Figure 5.16, performing a training with 120 epochs I found also in this case the same behavior and the same trend of the models, observed in the first experiment (Figure 5.17). 
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/cl_inverso_best.jpg}
\caption{Confusion Matrix for the new sequence of activities}
\end{figure}\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.35]{Images/trend_inverso.jpeg}
\caption{Predictions trend for the new sequence of activities}
\end{figure}

\section{Real AR dataset accuracy evaluation}
In this section the step from synthetic to real domain is shown. Best models (in terms of prediction overall accuracy) trained with synthetic dataset are tested on real datasets: in particular a custom dataset (captured by me in Konica Minolta) containing all our 3 activities and the UT-Kinect Action3D dataset (SOTA) from which I extracted the \emph{"Waving Hands"} activity.\\
I tested both dataset with the models we already seen in the first section of this chapter:\\
\begin{itemize}
    \item Classic Approach with CNN (model in Figure 5.2)
    \item Classic Approach with LSTM (model in Figure 5.6)
    \item EWC approach with CNN (model in Figure 5.12, epoch 64)
    \item EWC approach with CNN (model in Figure 5.16), just for SOTA dataset
\end{itemize}

\subsection{Custom dataset}
The custom dataset, for obvious reasons, is very limited: just 10 samples for each activity for a total of 30. However by testing my models I have found positive feedback, in particular with CNN approaches. We can see how the predicted values are in line with what we already seen for synthetic dataset. The CNN with classic approach (Figure 5.18 A) is stronger on Falling down, confusing Waving and Hands Up, with overall accuracy equal to 0.83. The CNN trained with EWC technique (Figure 5.18 B) instead has a lower accuracy, of course, very strong on the first activity (Waving Hands) as in synthetic domain. To get a clearer survey, however, more samples would be needed to test.\\
In the end we have the LSTM results too, showed in Figure 5.18(C): as we can deduce from the accuracy (0.63) coming from its Confusion Matrix, this model fell in overfitting during training, where it seemed to mark the three activities well. Instead it seems not able to distinguish Waving Hands and Hands Up.
\newpage
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/custom_cm_cnn.png} A
\includegraphics[scale=0.3]{Images/custom_cm_cl.png} B
\includegraphics[scale=0.3]{Images/custom_cm_lstm.png} C
\caption{Confusion Matrix for: (A) classic CNN, (B) CL CNN, (C) classic LSTM}
\end{figure}
\newpage
\subsection{SOTA dataset: UT-Kinect Action3D}
Nowadays a lot of SOTA datasets are available, dedicated to the collection of any kind of human activity, like UCF101, ActivityNet, Kinetics HAV, HMDB51, Florence2D/3D, NTU RGB+D, SYSU 3D and so on. We could continue on listing others for much longer; indeed the problem is another in this case: we need to find a dataset type \emph{"suitable"} for our trained model. In particular, this must: (1) be single-person; (2) contain at least one among "Waving", "Falling" and "Hands Up"; (3) contain at least 50 frames (which are required as input by our model). So this is a very complicated challenge, especially when performed in a short amount of time, like the one I had available.\\ After a careful scouting among all available datasets, I found the following: \emph{UT-Kinect Action3D}. Despite this is born for research purposes far from mine (implement depth of kinetic camera for HAR), I noticed that it featured numerous frames for the Waving Hands activity; so I extracted and sampled them for preprocessing (Chapter 4.1.3), creating a suitable dataset to be tested by my network. \\Furthermore, today this is one of the most tested paper in the field of Action/Activity Recognition. To name just a few, in \emph{"Optimized Skeleton-based Action Recognition via Sparsified Graph Regression"} proposes a graph regression-based GCN (GR-GCN) for skeleton-based action recognition, with the goal of capturing spatiotemporal variation in the data, achieving state-of-the-art performance (accuracy $\sim 98\%$) on datasets such as NTU RGB+D, UT-Kinect and SYSU 3D (Figure 5.19) widely used in this field.
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.8]{Images/sota_paper1.png}
\caption{Paper performance on SYSU-3D dataset}
\end{figure}\\
Then in \emph{"Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition"}, a deep progressive reinforcement learning (DPRL) method for action recognition in skeleton-based videos is implemented, which aims to distill the most informative frames and discard ambiguous ones in sequences to recognize actions using a graph CNN to capture the dependency between joints for action recognition. Here we have very high performance on the dataset in question, achieving an accuracy of 98.5, outperforming similar approaches (Figure 5.20).\\
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.65]{Images/sota_paper2.png}
\caption{Performance on UT-Kinect Action3D dataset of HAR papers over years, compared with the mentioned.}
\end{figure}\\
There are many others such as \emph{"Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group"} or \emph{"HAMLET: A Hierarchical Multimodal Attention-based Human Activity Recognition Algorithm"} that in general manage, with innovative approaches, to achieve excellent performance in the field of HAR. So it is clear that it makes no sense to try to beat in accuracy such models, already \emph{"super performing"}. \\\\\\\\The contribution in my work is another, that is be able to obtain good results testing the dataset (at least accuracy > 0.7), but actually using a model:
\begin{itemize}
    \item trained in the synthetic domain, therefore without "human effort" to compose the dataset;
    \item trained adding sequential activities thanks to Continual Learning, therefore using a totally innovative technique in HAR area.
\end{itemize}
So let's move now to the achieved results, by observing before the model trained with the order Waving - HandsUp - Falling and then passing to the other Falling - Waving - HandsUp. \\As we can see in Figure 5.21, this certifies that the trained model, although it learns new tasks in sequence, it does not lose the first one (Waving), performing very well on the SOTA dataset (accuracy = 0.95).
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/cl_sota1.jpg}
\caption{CM for Waving Hands in UT-Kinect dataset}
\end{figure}
\newpage Figure 5.22 instead shows the second experiment results, which demonstrates being able to learn the new activity (quite good) added in sequence with EWC, reaching an accuracy equal to 0.8.
\begin{figure}[ht]
\centering    
\includegraphics[scale=0.3]{Images/cl_sota2.jpg}
\caption{CM for Waving Hands in UT-Kinect dataset, with inverse order training }
\end{figure}\\
\\ Unfortunately both real datasets contains a very small number of samples, so it's obvious that we can't draw any definitive conclusions about what we have already seen.

\section{Discussion}
From the results obtained from the experiments, we showed that the approach of adding new tasks sequentially, with Continual Learning, produced good and interesting results. We can confidently infer that the network has learned to predict fairly well while avoiding the danger of overfitting. Obviously, the best results come from tasks that are not network agnostic, namely those in the synthetic domain (the implemented randomization will never be enough!). As for real activities, where each movement is performed in a totally different way, I still achieved good results since the network seems to have learned well to generalize the concept of activities and to distinguish between them. As said before, this model is not perfect, and we could further strengthen the system by adding new information in the training phase. For example we could think of using a hybrid dataset (synthetic + real) to greatly increase the accuracy of our model.



\chapter{Conclusions}
%\lettrine[lines=2, findent=3pt, nindent=0pt]{A}{}
In this work I have presented a new way to perform the problem of Human A...
I collected dataset...
I wrote from scratch networks...
I implemented the ewc...
I obtained very good results, despite all the faced problems, showing all the positive aspects to use CL instead of classical approaches.
Per ottenere risultati migliori si dovrebbe...
possibili future works 
Dangerous situations with weapon detection
Multiperson activitiy recognition i.e. for real alarm systems
CL lambda automatic hyperparametrization (KMLE publication)
Adding new activities and see effects
Testing on new SOTA dataset

\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}
Dear reader, if you have read my thesis till here, you should know that this was the hardest part to write. Hence, I say goodbye to the English language to switch to my native, thank you very much for the attention.
\\
\\ Prima di cominciare, voglio dire che sono davvero entusiasta di completare questo percorso di studi che rispecchia un mondo per il quale nutro una forte passione, che si è rivelata fondamentale per questo traguardo.
\\
\\ Vorrei dedicare qualche riga a tutti coloro che mi sono stati vicini in questo percorso di crescita personale e professionale e a tutte le persone che hanno contributo, ognuno a modo suo, alla realizzazione di questo mio grande traguardo. \\
\\Il primo rignraziamento, il più grande e sentito, va ovviamnte ai miei genitori, per avermi sostenuto e dato la possibilità di portare a termine gli studi universitari. Mi sento fortunato infatti ad avere una famiglia che ha sempre creduto in me nonostante il difficile percorso intrapreso e per questo non vi ringrazierò mai abbastanza.
\\
\\Ringrazio di cuore mia sorella Federica, che sebbene abbia un carattere opposto al mio è sempre la prima ad esultare per ogni mia gioia; insieme a Francesco mi ha continuamente supportato, dispensandomi saggi consigli di vita; a Francesco va anche un sentito secondo ringraziamento, il quale ha dimostrato di credere in me permettendomi di eseguire il tirocinio in azienda.
\\
\\Ringrazio poi tutti i miei parenti, siete troppi per potervi elencare tutti, ma sappiate che siete sempre nel mio cuore. 
\\
\\Grazie a Claudia, sempre al mio fianco e ancora di salvezza nei periodi difficili che non ha mai smesso di sostenermi, neanche un secondo. Gran parte dei miei successi sono dovuti a te, che sai trasmettermi tanta serenità, ma anche la giusta carica quando serve. Grazie di tutto.
\\
\\Grazie ai miei migliori amici, Fabio, Federico, Federico e Francesco, compagni di vita da sempre. Con voi ho condiviso mille avventure che hanno contribuito a rendere questi anni davvero indimenticabili.
\\
\\Grazie inoltre a quegli amici su cui sai che puoi contare anche se putroppo non riesci a vedere frequentemente, tra cui Gianluca, Francesca, Michele, Salvatore, Stefano e Federico. Vi voglio bene.
\\
\\Un grazie speciale va ai miei compagni di studi diventati ormai miei grandi amici, tra cui Michele e Davide; avete reso questi anni molto piacevoli e voi come nessun altro potete capire che significa diventare ingegneri. In particolare grazie a Michele, compagno di banco in questa vita universitaria, con il quale ho affrontato e superato qualunque ostacolo.
\\
\\Ringrazio poi tutto il team di Ricerca e Sviluppo di Konica Minolta, che in questi cinque mesi di lavoro sono stati miei colleghi di ogni giorno. Oltre ad essere tutti molto competenti vi siete rivelati belle persone, rendendo il mio tirocinio e permanenza in azienda un'esperienza stupenda. In particolare ringrazio Luca, che fin dall'inizio mi è stato accanto guidandomi saggiamente alla realizzazione del mio lavoro.
\\
\\Infine un ringraziamento sincero e di stima va anche al mio relatore, il professor Luca Iocchi, sempre disponibile per qualsiasi consiglo e chiarimento; è stato il primo professore che ho conosciuto in questo percorso magistrale e sono contento che sia stato lui a guidarmi alla sua conclusione. \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ \emph{Alla mia cara nonna Bianca, scomparsa un anno fa.}

\backmatter
\phantomsection
\begin{thebibliography}{17}

 \bibitem{ref:science}
 A. Aich, Elastic Weight Consolidation (EWC): Nuts and Bolts, Riverside, USA, 2020

 \bibitem{ref:science}
 D. Beddiar, B. Nini, M. Saboku et al Vision-based human activity recognition: a survey, Multimed Tools Appl

 \bibitem{ref:science}
 P. Kaushik, A. Gain, A. Kortylewski, A. Yuille, Understanding Catastrophic Forgetting and Remembering in Continual Learning with Optimal Relevance Mapping, 2021
 
 \bibitem{ref:science}
 O. Ostapenko, M. Puscas, T. Klein, P. Jahnichen, M. Nabi, Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning, Humboldt University Berlin, University of Trento, SAP ML Research, 2019

 \bibitem{ref:science}
 M. Sanzari, V. Ntouskos, F. Pirri, et al Discovery and recognition of motion primitives in human activities, La Sapienza, Rome, Italy

\bibitem{ref:science}
 S. W. Pienaar and R. Malekian et al Human Activity Recognition Using LSTM-RNN Deep Neural Network Architecture, Department of Electrical, Electronic and Computer Engineering, University of Pretoria, South Africa, 2019
 
 \bibitem{ref:science}
 E. Marinoiu, M. Zanfir, V. Olaru, C. Sminchisescu,3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children with Autism, Department of Mathematics, Faculty of Engineering, Lund University, Institute of Mathematics of the Romanian Academy
 
 \bibitem{ref:science}
 J. Kao, A. Ortega, D. Tian, H. Mansour, A. Vetro, Graph Based Skeleton Modeling for Human Activity Analysis, MITSUBISHI ELECTRIC RESEARCH LABORATORIES, 2019

\bibitem{ref:science}
 G. I. Parisi, R. Kemker, J. Part, C. Kanan, S. Wermter, et al Continual Lifelong Learning: A Review, Universitat Hamburg (Germany), Rochester Institute of Technology (USA), Edinburgh Centre for Robotics (UK)

\bibitem{ref:science}
 A. Juliani, V.Berges, E. Teng, A. Cohen, J. Harper, C. Elion, C. Goy, Y. Gao, H. Henry, M. Matter, D. Lange, Unity: A General Platform for Intelligent Agents, Unity Technologies, San Francisco, USA, 2020
 
 \bibitem{ref:science}
 K. Hamada, K. Tachibana, T. Li, H. Honda and Y. Uchida, Full-body High-resolution Anime Generation with Progressive Structure-conditional Generative Adversarial Networks, DeNA Co., Ltd., Tokyo, Japan, 2018
 
 \bibitem{ref:science}
 X. Li, K. W., Member, Y.n Tian, L. Yan, and F. Wang; The ParallelEye Dataset: Constructing Large-Scale Artificial Scenes for Traffic Vision Research, 2017
 
 \bibitem{ref:science}
 F. J. Ordóñez and D.R. et al Received  Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition, University of Sussex, UK, 2016
 
 \bibitem{ref:science}
 X. Gao, W. Hu, J. Tang, J. Liu, Z. Guo, Optimized Skeleton-based Action Recognition via Sparsified Graph Regression, Institute of Computer Science and Technology, Peking University, China, 2019
 
 \bibitem{ref:science}
 Y. Tang, Y. Tian, J. Lu, P. Li, J. Zhou, Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition, Department of Automation, Tsinghua University, China, State Key Lab of Intelligent Technologies and Systems, Tsinghua University, China
 
 \bibitem{ref:science}
 R. Slama, H. Wannous, M. Daoudi, A. Srivastava, Accurate 3D Action Recognition using Learning on the Grassmann Manifold, University of Lille, Villeneuve d’Ascq, France and Florida State University, Departement of Statistics, Tallahassee, USA
 
 \bibitem{ref:science}
 Chao Li, Qiaoyong Zhong, Di Xie, Shiliang Pu et al Skeleton-Basd Action Recognition With Convolutional Neural Networks, Hikvision Research Institute, Hangzhou, China, 2017
 
 \bibitem{ref:science}
 J. Shotton, R. Girshick, A. Fitzgibbon, and T. Sharp, “Efficient human pose estimation from single depth images” TPAMI, 2013
 
 \bibitem{ref:science}
 R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach and M. Tuytelaars, et al Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV) 2018
 
 \bibitem{ref:science}
 R. Vemulapalli, F. Arrate and R. Chellappa, Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group, Center for Automation Research UMIACS, University of Maryland, College Park
 
 \bibitem{ref:science}
 M. Leontev, V. Islenteva, and S. V. Sukhov, Non-Iterative Knowledge Fusion in Deep Convolutional Neural Networks, Ulyanovsk State Technical University, Russia, 2018
 
 \bibitem{ref:science}
 I. Lee, D. Kim, S. Kang, S. Lee et al Ensemble Deep Learning for Skeleton-based Action Recognition using Temporal Sliding LSTM networks, Department of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea
 
 \bibitem{ref:science}
 H. Duan, Y. Zhao, K. Chen, D. Shao, D. Lin, B. Dai, Revisiting Skeleton-based Action Recognition, The Chinese University of HongKong, The University of Texas at Austin, 2021
 
 \bibitem{ref:science}
 G. Paoletti, J. Cavazza, C. Beyan and A. Del Bue, Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning, Pattern Analysis and Computer Vision, Istituto Italiano di Tecnologia, Genova, Italy, 2020
 
 \bibitem{ref:science}
 Lu Xia, Chia-Chih Chen, and J. K. Aggarwal, View Invariant Human Action Recognition Using Histograms of 3D Joints, The University of Texas at Austin, USA, 2012
 
 \bibitem{ref:science}
 B. Ren, M. Liu, Runwei Ding, D Liu et al A Survey on 3D Skeleton-Based Action Recognition Using Learning Method, 2020
 
 \bibitem{ref:science}
 N. Suriani, S. Ahmad, M. Norzali, M. Razali Tomari, W. N. Zakaria et al Human activity recognition based on optimal skeleton joints using convolutional neural network, Faculty of Electrical and Electronics Engineering, University Tun Hussein Onn, Malaysia, 2018
 
 \bibitem{ref:science}
 B. L., Mingyi He, X. Cheng, Y. Chen, Y. Dai et al Skeleton Based Action Recognition Using Translation-Scale Invariant Image Mapping And Multi-Scale Deep CNN, Northwestern Polytechnical University, China, 2017 
 
 \bibitem{ref:science}
 S. Jha, M. Schiemer, J. Ye et al Continual Learning in Human Activity Recognition: an Empirical Analysis of Regularization
 
 \bibitem{ref:science}
 J. Kirkpatrick, R. Pascanua, N. Rabinowitz et al Overcoming catastrophic forgetting in neural networks, Imperial College, DeepMind London, UK
 
 \bibitem{ref:science}
 A. Kanazawa, M. J. Black, D. W. Jacobs, J. Malik et al End-to-end Recovery of Human Shape and Pose, Berkeley, USA
 
 \bibitem{ref:science}
 X. Zhang, J. Zhao, Y. LeCun, Character-level Convolutional Networks for Text Classification, Courant Institute of Mathematical Sciences, New York University, 2016
 
 \bibitem{ref:science}
 M. B. Shaikh and D. Chai 1, RGB-D Data-based Action Recognition: A Review, School of Engineering, Edith Cowan University, Perth, Australia 2021
 
 \bibitem{ref:science}
 S. Bai, J. Z. Kolter, V. Koltun, An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling, 2018
 
 \bibitem{ref:science}
 A. Guilbeault-Sauvé,B. De Kelper and J. Voix, Man Down Situation Detection Using an in-Ear Inertial Platform, Université du Québec, École de technologie supérieure (ÉTS), Montréal, QC H3C 1K3, Canada, 2021
 
 \bibitem{ref:science}
 A. Thai, S. Stojanov, I. Rehg, J. M. Rehg, "Does Continual Learning = Catastrophic Forgetting?", Georgia Institute of Technology, 2019

\end{thebibliography}


\chapter*{Sitography}
\addcontentsline{toc}{chapter}{Sitography}
\begin{itemize}
\item Deep Learning for HAR, \url{https://machinelearningmastery.com/deep-learning-models-for-human-activity-recognition}

\item Skeleton Estimation open-source code, \url{https://github.com/russoale/hmr2}

\item HAR using LSTM, \url{https://medium.com/@curiousily/human-activity-recognition-using-lstms-on-android-tensorflow-for-hackers-part-vi-492da5adef64}

\item Continuous Learning (medium), \url{https://medium.com/continual-ai/why-continuous-learning-is-the-key-towards-machine-intelligence-1851cb57c308}

\item CL PhD thesis, \url{http://amsdottorato.unibo.it/9073/1/vincenzo_lomonaco_thesis.pdf}

\item Unity Perception, \url{https://github.com/Unity-Technologies/com.unity.perception}

\item Personal Perception Unity documentation, \url{https://github.com/FlavioLorenzi/com.unity.perception/blob/master/com.unity.perception/Documentation~/HPETutorial/TUTORIAL.md}

\item Tutorial tensoflow serving, \url{https://www.tensorflow.org/tfx/tutorials/serving/rest_simple}

\item RNN for HAR, \url{https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification}

\item Unity 3D guide, \url{https://innovaformazioneblog.altervista.org/cose-unity-3d}

\item Prevent Catastrophic Forgetting (CL), \url{http://www.lherranz.org/2018/08/21/rotating-networks-to-prevent-catastrophic-forgetting}

\item HAR guide, \url{https://medium.datadriveninvestor.com/a-guide-to-human-activity-recognition-f11e4637dc4e}

\item Unity for Computer Vision,\url{https://blog.unity.com/technology/supercharge-your-computer-vision-models-with-synthetic-datasets-built-by-unity}

\item CNN and LSTM for text classification \url{https://medium.com/ai-ml-at-symantec/should-we-abandon-lstm-for-cnn-83accaeb93d6}

\item RNNs & LSTM survey,\url{https://colah.github.io/posts/2015-08-Understanding-LSTMs}

\item "Papers With Code", \url{https://paperswithcode.com}

\item Dataset Preprocessing, \url{https://www.javatpoint.com/data-preprocessing-machine-learning}
\end{itemize}

\end{document}