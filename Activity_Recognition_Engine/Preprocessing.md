# Preprocessing input data for activity recognition engine

## Steps done

### Collect Unity dataset for each activity (250 RGB frames + labels captures.json)

The run button cannot be called multiple times from Unity platform; so I used linux xdotool to repeat mouse movement and click 1k times the run button for each camera view (4). Obviously this is done for all the 3 activities, so we have 1000 x 4 x 3 samples = 12k.

### Convert Unity dataset to oid format

For each Activity I wrote and run unity2oid.py script to:

- Rename & sort the Unity (raw) dataset;
- Convert it to oid folder all into train_folder;
- Given config.ini specification compute the train/val split (80% - 20%).

Now we have the following format: 250 RGB images (total frame from Unity) + labels text files with bounding boxes.

### Convert oid dataset to our final preprocessed ar_dataset

For each Activity I wrote and run adapt_data_for_training.py script to:

- [Crop images given their bounding boxes](./mandown.gif);
- Sampling to specified frames number;
- Retrieve skeleton joints thanks to skeleton estimator (numpy vector sampleNumber x 50x57);
- Save them in h5 files;
- Save all into final ar_dataset folder.

Now we have the following dataset:

Activity / train-val / sample 00000x / file.h5 + cropped & original images
