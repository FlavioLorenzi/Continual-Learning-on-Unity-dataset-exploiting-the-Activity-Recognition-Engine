# AR engine

The objective of HAR is to detect and analyze human activities from the information acquired from sensors such a sequence of images, either captured by RGB cameras (Kinect, webcam), range sensors, or other sensing modalities. Most of the research work mainly focuses on a spatial and temporal aspect of a video sequence of action recognition captured by RGB cameras. However, the image may suffer from illumination, various points of view, self- occlusion problems, clutter background and body segmentation error. These are the issues of color videos which influence the accuracy performance, especially when dealing with complex human actions.

The engine was carried out step by step, putting together different works and results obtained, with the aim of creating a cascade structure (as shown below).

In particular the training papeline is realized from scratch, building a hybrid CNN-LSTM network that leverages the strengths of both.
.


_________________________________________

### TRAINING

INPUT (Unity synthetic dataset) (Activities: HANDS UP, WAVING HANDS, MAN DOWN)

[PREPROCESSING INPUT](./Preprocessing.md)

Custom LSTM/CNN (hybrid approach) training papeline

OUTPUT (model.pb)

__________________________________________

### INFERENCE


INPUT (Unity or real scenes) (Activities: HANDS UP, WAVING HANDS, MAN DOWN)

[Person detection engine (YOLOv4)](./PersonDetection/ReadMe.md) <--> Tensorflow serving (models.pb)

[Skeleton Estimation](./SkeletonEstimation/ReadMe.md) <--> Tensorflow serving (models.pb)

LSTM (with trained model) or CNN inference <--> Tensorflow serving (models.pb)

OUTPUT (for each scene recognize the activity)

