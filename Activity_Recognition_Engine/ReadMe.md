# AR engine

The objective of HAR is to detect and analyze human activities from the information acquired from sensors such a sequence of images, either captured by RGB cameras (Kinect, webcam), range sensors, or other sensing modalities. Most of the research work mainly focuses on a spatial and temporal aspect of a video sequence of action recognition captured by RGB cameras. However, the image may suffer from illumination, various points of view, self- occlusion problems, clutter background and body segmentation error. These are the issues of color videos which influence the accuracy performance, especially when dealing with complex human actions.

The engine was carried out step by step, putting together different works and results obtained, with the aim of creating a cascade structure (as shown below).

In particular the training papeline is realized from scratch, building a hybrid CNN-LSTM network that leverages the strengths of both.
.


_________________________________________

### TRAINING

INPUT (Unity synthetic dataset) (Activities: HANDS UP, WAVING HANDS, MAN DOWN)

[PREPROCESSING INPUT](./Preprocessing.md)

Custom LSTM & custom CNN training papeline

OUTPUT (model.pb)


### TEST

Offline prediction with blind dataset:

accuracy evaluation with Confusion Matrix generation

NB: the test dataset is very similar to the train dataset (synthetic), so results will be overfitted (with very high accuracy): to see real power of models we should test over real datasets (see later ---> one of the final aims of this work).

__________________________________________

### INFERENCE with tf serving


INPUT (Unity or real scenes) (Activities: HANDS UP, WAVING HANDS, MAN DOWN)

[Person detection engine (YOLOv4)](./PersonDetection/ReadMe.md) <--> Tensorflow serving (models.pb)

[Skeleton Estimation](./SkeletonEstimation/ReadMe.md) <--> Tensorflow serving (models.pb)

TODO: LSTM (with trained model) or CNN inference <--> Tensorflow serving (models.pb)

OUTPUT (for each scene recognize the activity)

