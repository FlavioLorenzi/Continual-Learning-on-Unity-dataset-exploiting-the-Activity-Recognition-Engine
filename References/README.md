# Continuous Learning State of Art

## Abstract

Achieving artificial general intelligence requires that agents are able to learn and remember many different tasks. This is particularly difficult in real-world settings: The sequence of tasks may not be explicitly labeled, tasks may switch unpre- dictably, and any individual task may not recur for long time intervals. Critically, therefore, intelligent agents must demon- strate a capacity for continual learning: that is, the ability to learn consecutive tasks without forgetting how to perform previously trained tasks.
Continual learning poses particular challenges for artificial neural networks due to the tendency for knowledge of the previously learned tasks (e.g., task A) to be abruptly lost as information relevant to the current task (e.g., task B) is incorporated. This phenomenon, termed catastrophic forgetting, occurs specifically when the network is trained sequentially on multiple tasks because the weights in the network that are important for task A are changed to meet the objectives of task B.


Whereas recent advances in machine learning and in particular deep neural networks have resulted in impressive gains in performance across a variety of domains, little progress has been made in achieving continual learning. Current approaches have typically ensured that data from all tasks are simultaneously available during training. This approach [often called system-level consolidation] is impractical for learning large numbers of tasks, as in our setting it would require the amount of memories being stored and replayed to be proportional to the number of tasks. The lack of algorithms to support continual learning thus remains a key barrier to the development of artificial general intelligence.


In marked contrast to artificial neural networks, humans and other animals appear to be able to learn in a continual fashion. Recent evidence suggests that the mammalian brain may avoid catastrophic forgetting by protecting previously acquired knowledge in neocortical circuits. When a mouse acquires a new skill, a proportion of excitatory synapses are strengthened; this manifests as an increase in the volume of individual dendritic spines of neurons. Critically, these enlarged dendritic spines persist despite the subsequent learning of other tasks, accounting for retention of performance several months later. When these spines are selectively “erased,” the corresponding skill is forgotten. This provides causal evidence that neural mechanisms supporting the protection of these strengthened synapses are critical to retention of task per- formance. These experimental findings, together with neurobiological models suggest that continual learning in the neocortex relies on task-specific synaptic consolidation, whereby knowledge is durably encoded by rendering a proportion of synapses less plastic and therefore stable over long timescales.


So it is demonstrated that the task-specific synaptic consolidation offers a unique solution to the continual-learning problem for artificial intelligence. We will see an algorithm analogous to synaptic consolidation for artificial neural networks, which we refer to as elastic weight consolidation (EWC). This algorithm slows down learning on certain weights based on how important they are to previously seen tasks. 



## EWC algorithm vs Catastrophic Forgetting

A deep neural network consists of multiple layers of lin- ear projection followed by element-wise nonlinearities. Learning a task consists of adjusting the set of weights and biases θ of the linear projections, to optimize performance. Many configurations of θ will result in the same performance; this overparameterization makes it likely that there is a solution for task B, θB∗ , that is close to the previously found solu- tion for task A, θA∗ . While learning task B, EWC therefore protects the performance in task A by constraining the param- eters to stay in a region of low error for task A centered around θA∗ , as shown schematically in the figure below. This constraint is imple- mented as a quadratic penalty and can therefore be imagined as a spring anchoring the parameters to the previous solution, hence having the name elastic. Importantly, the stiffness of this spring should not be the same for all parameters; rather, it should be greater for parameters that most affect performance in task A. To justify this choice of constraint and to define which weights are most important for a task, it is useful to consider neural network training from a probabilistic perspective. From this point of view, optimizing the parameters is tantamount to finding their most probable values given some data D (Bayes Rule).

<p align="center">
 <img src="ewc.png" width="250" height="250">
 </p>







-----------

### referenced paper
Overcoming catastrophic forgetting in neural networks, DeepMind, London EC4 5TW, United Kingdom; and bBioengineering Department, Imperial College London, London SW7 2AZ, United Kingdom
